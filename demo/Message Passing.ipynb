{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNOzybE3gN63bUrR++DK14Z"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# Message Passing\n","\n","本章節會介紹如何使在 DGL 建立 Message Passing 和 GNN 層\n","\n","同樣地，我們先檢查 cuda 版本和安裝 dgl 庫"],"metadata":{"id":"PWbmjzqUF1B6"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nxBVUi3HFtIy","executionInfo":{"status":"ok","timestamp":1689277061029,"user_tz":-480,"elapsed":337,"user":{"displayName":"Dong Dong","userId":"17680164657657523368"}},"outputId":"5b6577fa-f02a-4429-dc58-430d156047a9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Thu Jul 13 19:37:48 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   54C    P8    10W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","source":["!pip install  dgl -q -f https://data.dgl.ai/wheels/cu118/repo.html\n","!pip install  dglgo -q -f https://data.dgl.ai/wheels-test/repo.html"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tX1jhM9cGcPN","executionInfo":{"status":"ok","timestamp":1689277116387,"user_tz":-480,"elapsed":55071,"user":{"displayName":"Dong Dong","userId":"17680164657657523368"}},"outputId":"b28d088c-d0cc-4845-ca77-b1da2ac38ac4"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.5/63.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.4/52.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.2/112.2 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.4/29.4 MB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m85.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.6/485.6 kB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m572.7/572.7 kB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"code","source":["import os\n","os.environ[\"DGLBACKEND\"] = \"pytorch\"\n","\n","import dgl\n","import torch\n","import torch.nn.functional as F"],"metadata":{"id":"p8zJ3y_HGGDS","executionInfo":{"status":"ok","timestamp":1689277122337,"user_tz":-480,"elapsed":5953,"user":{"displayName":"Dong Dong","userId":"17680164657657523368"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["## What is Message Passing?\n","\n","Message Passing 是一種神經網路的信息傳遞機制，透過信息傳遞，我們可以在網路中進行信息的共享，在 CV 或 NLP 中，我們可以常常見到類似的概念。Message Passing 主要包含兩步驟\n","\n","1. 從周圍的點計算信息\n","2. 將計算的信息進行匯聚\n","\n","在 CV 中的 CNN 模塊也可以看做是一個 Message Passing 機制，對於每一個 pixel，如果我們使用 3*3 的捲積核，我們會根據濾波器的權重將該 pixel 周圍八個相鄰點的特徵做加權平均，作為這個 pixel 新的特徵，而該特徵為聚合了周圍所有鄰居的信息\n","\n","在圖上也一樣，我們可以透過相同的方式做到一樣的事情，我們把一個點的周圍相鄰點 (點向量) 信息做匯聚，但是在這裡，匯聚的權重通常使用 mean, sum, max 等方式。所以**圖上的信息傳遞其實可以看成 CNN 上的運算**，只是每個 pixel 考慮的權重一樣而已\n","\n","![](https://hackmd.io/_uploads/HJR15lSSh.png)\n","\n","### Graph Embeddings\n","\n","在進入到真正的 Message Passsing 之前，我們先來講講怎麼做圖的 embeddings。回想我們對於點、邊和圖的信息使用矩陣的方式儲存，並用列表儲存圖的連接性，理所當然，我們可以用參數矩陣來對圖的信息做 embeddings。\n","\n","我們將分為三個面向討論 GNN，我們假設第 $t$ 層的對於點、邊和圖的參數矩陣為 $(W^{(t)}_v, W^{(t)}_e, W^{(t)}_g)$，embeddings 向量為 $(v^{(t)}, e^{(t)}, g^{(t)})$，則我們可以通過一種簡單的現性變換得到圖的 embeddings，具體透過下面的方式可以得到\n","\n","$$\n","\\begin{align*}\n","v^{(t+1)}&=W^{(t)}_vv^{(t)} \\\\\n","e^{(t+1)}&=W^{(t)}_ee^{(t)} \\\\\n","g^{(t+1)}&=W^{(t)}_gg^{(t)} \\\\\n","\\end{align*}\n","$$\n","\n","其中 $(W^{(t)}_v, W^{(t)}_e, W^{(t)}_g)$ 可以用梯度下降找出。這就是最基本的 GNN 模塊，我們可以使用該模塊得到圖的潛在信息表徵，這類似於 CV 中的 CNN backbone 用來提取影像特徵，或者是 NLP 中的 BERT backbone 用來提取文字特徵。GNN layer 提取到圖上的特徵之後，我們可以根據提取到的特徵做我們感興趣的**下游任務**\n","\n","#### Information sharing\n","\n","注意到在我們剛剛的討論中完全沒有用到圖的優勢，換句話說，$(v^{(t)}, e^{(t)}, g^{(t)})$ 完全沒有進行信息交互。要怎麼進行信息交互呢? 這裡就可以提到 message passing 的精神了。假設我們想要得到 $v^{(t+1)}$ 的 embeddings，我們可以把與 $v$ 相鄰的點考慮進來，記做 $\\mathcal{N}(v)$，我們通過一個匯聚函數 $\\rho$ 把周圍信息匯聚起來，在通過更新函數 $\\psi$ 將原本的 $v^{(t)}$ 做更新，具體如下\n","\n","$$\n","\\begin{align*}\n","m_v^{(t+1)}&=\\{W^{(t)}_vv^{(t)}| v\\in \\mathcal{N}(v)\\}\\\\\n","v^{(t+1)}&=\\psi(v^{(t)}, \\rho(m_v^{(t+1)}))\n","\\end{align*}\n","$$\n","\n","$\\rho$ 函數通常會選擇 mean, sum 或 max 等方式，為了方便起見，我們把上述整個流程用 $\\rho_{V\\to V}$ 表示\n","\n","是不是覺得上面這個公式很複雜，但這還算是基本的匯聚方式，我們大可以考慮更複雜的情況，上面的例子是匯聚點的鄰居，同樣地，也可以把邊和圖的信息匯聚過來。聽起來很容易吧，但是，當我的點向量 embeddings dimension 和邊向量 embeddings 不一致怎麼辦?\n","\n","其實這個問題我們應該已經遇過很多次了，回想在 CV 或者 NLP 中我們是怎麼處理的，解決方法很簡單，一種方法是做向量拼接，不過這裡我們介紹另一種方法，只需要另外設定一個參數矩陣用來做現性變換即可，我們記做 $W_{e\\to v}^{(t)}$，具體方法如下\n","\n","$$\n","\\begin{align*}\n","e^{(t)}&=W_{e\\to v}^{(t)}W^{(t)}_ee^{(t)} \\\\\n","m_v^{(t+1)}&=\\{W^{(t)}_vv^{(t)}| v\\in \\mathcal{N}(v)\\}\\cup\\{e^{(t)}|e\\in\\mathcal{N}(v)\\}\\\\\n","v^{(t+1)}&=\\psi(v^{(t)}, \\rho(m_v^{(t+1)}))\n","\\end{align*}\n","$$\n","\n","我們把整個過程記做 $\\rho_{E\\to V}$ 代表把邊的信息匯聚到點上，同樣的我們可以有 $\\rho_{E\\to E}, \\rho_{E\\to G}, \\rho_{G\\to G}, \\rho_{V\\to E}, \\rho_{V\\to G}, \\rho_{E\\to V}, \\rho_{V\\to V}, \\rho_{G\\to V}$ 等方式，更複雜的說，在一層 GNN 中，我們可以有好幾次的信息傳遞，或者傳遞有先後順序，如下圖\n","\n","![](https://hackmd.io/_uploads/ry2BmGBSh.png)\n","\n","我們可以隨意的傳遞信息"],"metadata":{"id":"RFP-KsjxHLCj"}},{"cell_type":"markdown","source":["## Message Passing in DGL\n","\n","我們接下來來實戰 DGL 中的 Message Passing。假設我們想知道點 $v$ 在 $t+1$ 層的 embeddings vector $x_v\\in \\mathbb{R}^{d_1}$，邊 $(u, v)$ 的特徵 $w_e\\in\\mathbb{R}^{d_2}$，則 DGL 中定義的消息傳遞泛式為\n","\n","$$\n","m_e^{(t+1)}=\\phi(x_v^{(t)}, x_u^{(t)}, w_e^{(t)}), (u,v,e)\\in\\mathcal{E}\\\\\n","x_v^{(t+1)}=\\psi\\{x_v^{(t)},\\rho(\\{m_e^{(t+1)}:(u,v,e)\\in\\mathcal{E}\\})\\}\n","$$\n","\n","在上面的公式中\n","\n","- $\\phi:$ 定義在每條邊上的消息函數，通過邊兩端的端點來結合生成消息\n","  - 參數 `edges` 為 `EdgeBatch` 實例\n","  - `edges` 有 `src`, `dst`, `data` 三個屬性，`data` 表示邊的特徵\n","- $\\rho:$ 聚合函數來聚合節點的消息\n","  - 參數 `nodes` 為 `NodeBatch` 實例\n","  - `nodes` 有 `mailbox` 儲存節點收到的信息\n","- $\\psi:$ 更新函數結合更新後的消息和本身的信息\n","  - 參數 `nodes` 同上，作用在 $\\rho$ 函數之後\n","\n","當然我們可以自己定義我們的 $\\phi$ 如下"],"metadata":{"id":"9ImIvdW0mVxd"}},{"cell_type":"code","source":["def message_func(edges):\n","    return {'m': edges.src['hu'] + edges.dst['hv']}"],"metadata":{"id":"AGJFGLHqG6us","executionInfo":{"status":"ok","timestamp":1689277122338,"user_tz":-480,"elapsed":4,"user":{"displayName":"Dong Dong","userId":"17680164657657523368"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["該函數定義了將邊的 src 特徵 `hu` 和 dst 特徵 `hv` 做加總\n","\n","同樣地，也可以定義 $\\rho$"],"metadata":{"id":"0LMnx6Vnq5ht"}},{"cell_type":"code","source":["def reduce_func(nodes):\n","    return {'h': torch.sum(nodes.mailbox['m'], dim=1)}"],"metadata":{"id":"k9pqo0b8quu3","executionInfo":{"status":"ok","timestamp":1689277122338,"user_tz":-480,"elapsed":3,"user":{"displayName":"Dong Dong","userId":"17680164657657523368"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["有了上面的兩個函數 (dgl 不推薦使用者定義 $\\psi$ 函數)，我們就可以把整個消息傳遞函數透過 `update_all` API 寫出來了"],"metadata":{"id":"Q8cbosSqrjlF"}},{"cell_type":"code","source":["import dgl.function as fn\n","def update_all_example(graph):\n","    # 在graph.ndata['ft']中存储结果\n","    graph.update_all(message_func, reduce_func)\n","    # 在update_all外调用更新函数\n","    final_ft = graph.ndata['h'] * 2\n","    return final_ft"],"metadata":{"id":"ok9Q7u3IrjNv","executionInfo":{"status":"ok","timestamp":1689277122338,"user_tz":-480,"elapsed":3,"user":{"displayName":"Dong Dong","userId":"17680164657657523368"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["執行完上述過程後，dgl 會自動把消息 $m$ 歸零重置。當然如果我們不想使用 message passing，只想要讓資料在邊上傳遞，不進行與點或者圖的交互，我們可以使用 `apply_edges(func)`，其中 `func` 為 $\\phi$ 函數，也就是要對每條邊做什麼變換，這個 API 會對所有的邊進行更新，用法如下"],"metadata":{"id":"bW7VB8N4sJcs"}},{"cell_type":"code","source":["# g.apply_edges(fn.u_add_v('el', 'er', 'e'))\n","# It is equivalent to\n","# def message_func(edges):\n","#      return {'e': edges.src['el'] + edges.dst['er']}"],"metadata":{"id":"CeUbT37iuCp1","executionInfo":{"status":"ok","timestamp":1689277122338,"user_tz":-480,"elapsed":3,"user":{"displayName":"Dong Dong","userId":"17680164657657523368"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["在 dgl 中\n","\n","- `u:` 即 src\n","- `v:` 即 dst\n","- `e:` 即 (src, dst)\n","\n","`u_add_v` 接口表示把 src 特徵 `el` 的值和 dst 特徵 `er` 做和，並命名為 `e`\n","\n","Note: 最好是使用 `u_func_v` 這種接口 API，會比自訂義函數來的高效"],"metadata":{"id":"gdntZk16uNGE"}},{"cell_type":"markdown","source":["我們來看一段簡單的例子，幫我們更了解 dgl 消息傳遞機制"],"metadata":{"id":"0WUa7PIbtoGU"}},{"cell_type":"code","source":["g = dgl.graph((\n","    [1, 3, 5, 0, 4, 2, 3, 3, 4, 5],\n","    [1, 1, 0, 0, 1, 2, 2, 0, 3, 3]\n","  ))\n","g.edata['eid'] = torch.arange(10)\n","def reducer(nodes):\n","    print(nodes.nodes())\n","    print(nodes.mailbox['eid'])\n","    return {'n': nodes.mailbox['eid'].sum(1)}\n","g.update_all(fn.copy_e('eid', 'eid'), reducer)\n","g.ndata"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jnnukd3EsJG3","executionInfo":{"status":"ok","timestamp":1689278214561,"user_tz":-480,"elapsed":292,"user":{"displayName":"Dong Dong","userId":"17680164657657523368"}},"outputId":"2af1c51e-bf5a-44e0-c43a-4933090a0ab2"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([2, 3])\n","tensor([[5, 6],\n","        [8, 9]])\n","tensor([0, 1])\n","tensor([[2, 3, 7],\n","        [0, 1, 4]])\n"]},{"output_type":"execute_result","data":{"text/plain":["{'n': tensor([12,  5, 11, 17,  0,  0])}"]},"metadata":{},"execution_count":41}]},{"cell_type":"code","source":["g = dgl.graph(([0, 1, 2, 3], [1, 2, 3, 4]))\n","g.ndata['x'] = torch.ones(5, 2)\n","g.update_all(fn.copy_u('x', 'm'), fn.sum('m', 'h'))\n","g.ndata['h']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0kZxi7SL060p","executionInfo":{"status":"ok","timestamp":1689278200100,"user_tz":-480,"elapsed":295,"user":{"displayName":"Dong Dong","userId":"17680164657657523368"}},"outputId":"7c96c98f-89cd-4c8a-b0f8-fcda09253295"},"execution_count":39,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0., 0.],\n","        [1., 1.],\n","        [1., 1.],\n","        [1., 1.],\n","        [1., 1.]])"]},"metadata":{},"execution_count":39}]},{"cell_type":"markdown","source":["綜合來講，dgl 把消息傳遞機制分為三個步驟\n","\n","1. 定義 $\\phi$\n","2. 定義 $\\rho$\n","3. 定義 $\\psi$"],"metadata":{"id":"_BX8EMEQ0U3f"}},{"cell_type":"code","source":["# 定義 ϕ 函數\n","def message_func(edges):\n","    return {'msg': edges.src['feat']}\n","\n","# 定義 ρ 函數\n","def reduce_func(nodes):\n","    return {'agg_feat': torch.sum(nodes.mailbox['msg'], dim=1)}\n","\n","# 定義 Message Passing 函數\n","def message_passing(g, node_feats):\n","    # 將節點特徵設置為圖的節點屬性\n","    g.ndata['feat'] = node_feats\n","\n","    # 執行 Message Passing\n","    g.update_all(message_func, reduce_func)\n","\n","    # 定義 ψ 函數\n","    updated_feats = g.ndata['agg_feat']\n","\n","    return updated_feats\n"],"metadata":{"id":"DhoGY3f70mK4","executionInfo":{"status":"ok","timestamp":1689277122832,"user_tz":-480,"elapsed":6,"user":{"displayName":"Dong Dong","userId":"17680164657657523368"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["### Efficient way to pass message\n","\n","我們現在已經知道怎麼在圖上進行消息傳遞了，但是這種方法通常不是最高效的\n","\n","假設我現在有一個任務如下\n","\n","- $\\phi(u,v): \\text{cat}(u,v)$，拼接 src 和 dst 特徵\n","- 只在邊上做計算\n","- 做 edge classification，目標為 3 類\n","\n","這個任務可以用以下方式實現"],"metadata":{"id":"3v4GkIP-vc-r"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","node_feat_dim = 3\n","out_dim = 3\n","g = dgl.graph([(0,1), (1,2), (3,5)])\n","g.ndata[\"feat\"] = torch.randn(6, 3)\n","\n","linear = nn.Parameter(torch.FloatTensor(size=(node_feat_dim * 2, out_dim)))\n","def concat_message_function(edges):\n","     return {'cat_feat': torch.cat([edges.src['feat'], edges.dst['feat']], dim=1)}\n","g.apply_edges(concat_message_function)\n","g.edata['out'] = g.edata['cat_feat'] @ linear"],"metadata":{"id":"wKIF7DA4vcm6","executionInfo":{"status":"ok","timestamp":1689277122832,"user_tz":-480,"elapsed":5,"user":{"displayName":"Dong Dong","userId":"17680164657657523368"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["但是這種方法不是高效的，比較好的方法是進行矩陣拆解的運算\n","\n","$$\n","W\\times\\text{cat}(u,v) = W_l\\times u+W_r\\times v\n","$$\n","\n","其中 $W_l$ 為 $W$ 的左半部分，$W_r$ 為右半部分"],"metadata":{"id":"ozChvRDsx6_b"}},{"cell_type":"code","source":["linear_src = nn.Parameter(torch.FloatTensor(size=(node_feat_dim, out_dim)))\n","linear_dst = nn.Parameter(torch.FloatTensor(size=(node_feat_dim, out_dim)))\n","out_src = g.ndata['feat'] @ linear_src\n","out_dst = g.ndata['feat'] @ linear_dst\n","g.srcdata.update({'out_src': out_src})\n","g.dstdata.update({'out_dst': out_dst})\n","g.apply_edges(fn.u_add_v('out_src', 'out_dst', 'out'))"],"metadata":{"id":"4xz7LUULrK8K","executionInfo":{"status":"ok","timestamp":1689277122832,"user_tz":-480,"elapsed":5,"user":{"displayName":"Dong Dong","userId":"17680164657657523368"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["這兩種方法是等價的，後面這種由於內存占用較小，因此是比較高效的"],"metadata":{"id":"QZeSnjzeyrTE"}},{"cell_type":"markdown","source":["### DGL on Heterograph\n","\n","接下來我們介紹怎麼在異構圖中做消息傳遞，主要分為兩個步驟\n","\n","1. 對不同類型的 edge 計算聚合消息\n","2. 對每個節點具和不同類型 edge 的消息\n","\n","在異構圖中，我們使用 API `multi_update_all`，該方法實現邏輯與 `update_all` 近乎相同，唯獨輸入為一個字典，必須對於不同類型的點或邊給與不同的 message passing 方法"],"metadata":{"id":"JFt3-YDszQUL"}},{"cell_type":"code","source":["import dgl\n","import dgl.function as fn\n","import torch\n","\n","def message_passing(g, node_feats, weight):\n","    funcs = {}\n","    for srctype, etype, dsttype in g.canonical_etypes:\n","        Wh = weight[etype](node_feats[srctype])\n","        g.nodes[srctype].data['Wh_%s' % etype] = Wh\n","        funcs[etype] = (fn.copy_u('Wh_%s' % etype, 'm'), fn.mean('m', 'h'))\n","\n","    g.multi_update_all(funcs, 'sum')\n","\n","    updated_feats = {ntype: g.nodes[ntype].data['h'] for ntype in g.ntypes}\n","    return updated_feats\n","\n","# 創建異質圖\n","g = dgl.heterograph({\n","    ('user', 'follows', 'user'): ([0, 1, 2, 3], [1, 2, 3, 4]),\n","    ('user', 'likes', 'item'): ([0, 1, 2, 3], [0, 1, 2, 3])\n","})\n","\n","# 創建節點特徵和權重\n","node_feats = {\n","    'user': torch.tensor([[0.1], [0.2], [0.3], [0.4], [0.5]]),\n","    'item': torch.tensor([[1.0], [2.0], [3.0], [4.0]])\n","}\n","\n","weight = {\n","    'follows': torch.nn.Linear(1, 1),\n","    'likes': torch.nn.Linear(1, 1)\n","}\n","\n","# 執行消息傳遞\n","updated_feats = message_passing(g, node_feats, weight)\n","\n","# 輸出更新後的節點特徵\n","for ntype, feats in updated_feats.items():\n","    print(f\"{ntype} features:\")\n","    print(feats)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rCcPp2jgywW1","executionInfo":{"status":"ok","timestamp":1689277122832,"user_tz":-480,"elapsed":4,"user":{"displayName":"Dong Dong","userId":"17680164657657523368"}},"outputId":"a6de6870-c50a-4123-e1f9-a68719747d83"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["item features:\n","tensor([[-0.8167],\n","        [-0.7368],\n","        [-0.6568],\n","        [-0.5768]], grad_fn=<DivBackward0>)\n","user features:\n","tensor([[0.0000],\n","        [0.4501],\n","        [0.3697],\n","        [0.2893],\n","        [0.2089]], grad_fn=<DivBackward0>)\n"]}]},{"cell_type":"markdown","source":["接下來的章節，我們將會介紹怎麼使用 message passing 建立 GNN 模塊"],"metadata":{"id":"DHd7hBmY1qg_"}},{"cell_type":"markdown","source":["## DGL GNN 模塊構造\n","\n","在 DGL GNN 模塊中，我們可以用不同後端搭建模型，dgl 除了消息傳遞方式之外，其他對於圖的操作基本上與深度學習框架是一致的，在這篇文章中，我們主要使用 Pytorch 作為後端引擎\n","\n","要構建自己的 GNN 模況，用戶必須指定以下幾個參數\n","\n","- `__init__:` python 中物件的內置魔法方法，該方法會在類別被實例化時執行，通常用於定義 GNN 模型架構\n","- `forward(self, graph, feat):` 指定輸入 GNN 模塊的資料，預設有兩個輸入\n","  - `graph:` `dgl.DGLGraph` 實例，表示圖的結構信息\n","  - `feat:` `tensor`，表示點的特徵\n","\n"],"metadata":{"id":"Ib_rm5O8PFc2"}},{"cell_type":"code","source":["import torch.nn as nn\n","from dgl.utils import expand_as_pair, check_eq_shape\n","\n","class SAGEConv(nn.Module):\n","    def __init__(self,\n","                 in_feats,\n","                 out_feats,\n","                 aggregator_type,\n","                 feat_drop=0.,\n","                 bias=True,\n","                 norm=None,\n","                 activation=None):\n","        super(SAGEConv, self).__init__()\n","\n","        self._in_src_feats, self._in_dst_feats = expand_as_pair(in_feats)\n","        self._out_feats = out_feats\n","        self._aggre_type = aggregator_type\n","        self.norm = norm\n","        self.feat_drop = nn.Dropout(feat_drop)\n","        self.activation = activation\n","        # aggregator type: mean/pool/lstm/gcn\n","        if aggregator_type == 'pool':\n","            self.fc_pool = nn.Linear(self._in_src_feats, self._in_src_feats)\n","        if aggregator_type == 'lstm':\n","            self.lstm = nn.LSTM(self._in_src_feats, self._in_src_feats, batch_first=True)\n","        if aggregator_type != 'gcn':\n","            self.fc_self = nn.Linear(self._in_dst_feats, out_feats, bias=bias)\n","        self.fc_neigh = nn.Linear(self._in_src_feats, out_feats, bias=bias)\n","        # 初始化參數\n","        self.reset_parameters()\n","\n","    def reset_parameters(self):\n","        \"\"\"初始化参数\n","        gain 為不同 activation function 的建議起始值\n","        \"\"\"\n","        gain = nn.init.calculate_gain('relu')\n","        if self._aggre_type == 'pool':\n","            nn.init.xavier_uniform_(self.fc_pool.weight, gain=gain)\n","        if self._aggre_type == 'lstm':\n","            self.lstm.reset_parameters()\n","        if self._aggre_type != 'gcn':\n","            nn.init.xavier_uniform_(self.fc_self.weight, gain=gain)\n","        nn.init.xavier_uniform_(self.fc_neigh.weight, gain=gain)\n","\n","    def _lstm_reducer(self, nodes):\n","        \"\"\"LSTM reducer\n","        NOTE(zihao): lstm reducer with default schedule (degree bucketing)\n","        is slow, we could accelerate this with degree padding in the future.\n","        \"\"\"\n","        m = nodes.mailbox['m'] # (B, L, D)\n","        batch_size = m.shape[0]\n","        h = (m.new_zeros((1, batch_size, self._in_src_feats)),\n","             m.new_zeros((1, batch_size, self._in_src_feats)))\n","        _, (rst, _) = self.lstm(m, h)\n","        return {'neigh': rst.squeeze(0)}\n","\n","    def forward(self, graph, feat):\n","        \"\"\" SAGE 模塊的前向傳播\n","        graph: DGLGraph 實例\n","        feat: pytorch tensor, 表示點的特徵\n","        \"\"\"\n","        # local_var 會創建一個局部的圖實例，這個實例與原圖相同，但在圖上修信息不會影像到\n","        # 全局的圖\n","        # 該方法可以用 with graph.local_scope: 代替\n","        graph = graph.local_var()\n","\n","        if isinstance(feat, tuple):\n","            # 若是包含 src, dst 信息的點，我們分別對點做 dropout\n","            feat_src = self.feat_drop(feat[0])\n","            feat_dst = self.feat_drop(feat[1])\n","        else:\n","            # 若是僅含點的信息，我們對點做 dropout\n","            feat_src = feat_dst = self.feat_drop(feat)\n","\n","        h_self = feat_dst\n","\n","        # copy_src 把源節點的特徵複製到目標節點上\n","        if self._aggre_type == 'mean':\n","            graph.srcdata['h'] = feat_src\n","            graph.update_all(fn.copy_src('h', 'm'), fn.mean('m', 'neigh'))\n","            h_neigh = graph.dstdata['neigh']\n","        elif self._aggre_type == 'gcn':\n","            check_eq_shape(feat)\n","            graph.srcdata['h'] = feat_src\n","            graph.dstdata['h'] = feat_dst     # same as above if homogeneous\n","            graph.update_all(fn.copy_src('h', 'm'), fn.sum('m', 'neigh'))\n","            # divide in_degrees\n","            degs = graph.in_degrees().to(feat_dst)\n","            h_neigh = (graph.dstdata['neigh'] + graph.dstdata['h']) / (degs.unsqueeze(-1) + 1)\n","        elif self._aggre_type == 'pool':\n","            graph.srcdata['h'] = F.relu(self.fc_pool(feat_src))\n","            graph.update_all(fn.copy_src('h', 'm'), fn.max('m', 'neigh'))\n","            h_neigh = graph.dstdata['neigh']\n","        elif self._aggre_type == 'lstm':\n","            graph.srcdata['h'] = feat_src\n","            graph.update_all(fn.copy_src('h', 'm'), self._lstm_reducer)\n","            h_neigh = graph.dstdata['neigh']\n","        else:\n","            raise KeyError('Aggregator type {} not recognized.'.format(self._aggre_type))\n","\n","        # GraphSAGE GCN does not require fc_self.\n","        if self._aggre_type == 'gcn':\n","            rst = self.fc_neigh(h_neigh)\n","        else:\n","            rst = self.fc_self(h_self) + self.fc_neigh(h_neigh)\n","        # activation\n","        if self.activation is not None:\n","            rst = self.activation(rst)\n","        # normalization\n","        if self.norm is not None:\n","            rst = self.norm(rst)\n","        return rst"],"metadata":{"id":"z5Z5NEkqSeWx","executionInfo":{"status":"ok","timestamp":1689277122832,"user_tz":-480,"elapsed":3,"user":{"displayName":"Dong Dong","userId":"17680164657657523368"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["class GraphSAGE(nn.Module):\n","    def __init__(self,\n","                 g,\n","                 in_feats,\n","                 n_hidden,\n","                 n_classes,\n","                 n_layers,\n","                 activation,\n","                 dropout,\n","                 aggregator_type):\n","        super(GraphSAGE, self).__init__()\n","        self.layers = nn.ModuleList()\n","        self.g = g\n","        # input layer\n","        self.layers.append(SAGEConv(in_feats, n_hidden, aggregator_type,\n","                                    feat_drop=dropout, activation=activation))\n","        # hidden layers\n","        for i in range(n_layers - 1):\n","            self.layers.append(SAGEConv(n_hidden, n_hidden, aggregator_type,\n","                                        feat_drop=dropout, activation=activation))\n","        # output layer\n","        self.layers.append(SAGEConv(n_hidden, n_classes, aggregator_type,\n","                                    feat_drop=dropout, activation=None)) # activation None\n","\n","    def forward(self, features):\n","        h = features\n","        for layer in self.layers:\n","            h = layer(self.g, h)\n","        return h"],"metadata":{"id":"UbxBUm1skbpT","executionInfo":{"status":"ok","timestamp":1689277122833,"user_tz":-480,"elapsed":4,"user":{"displayName":"Dong Dong","userId":"17680164657657523368"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["接下來我們看看在異構圖上的 GNN 模塊怎麼定義，我們具體會有以下幾種步驟\n","\n","1. 為不同關係定義不同的 GNN 模塊\n","2. 聚合不同關係的結果"],"metadata":{"id":"PohE5taLlq7s"}},{"cell_type":"code","source":["\"\"\"Heterograph NN modules\"\"\"\n","from functools import partial\n","\n","import torch as th\n","import torch.nn as nn\n","\n","from dgl.base import DGLError\n","\n","__all__ = [\"HeteroGraphConv\", \"HeteroLinear\", \"HeteroEmbedding\"]\n","\n","class HeteroGraphConv(nn.Module):\n","    r\"\"\"A generic module for computing convolution on heterogeneous graphs.\n","\n","    The heterograph convolution applies sub-modules on their associating\n","    relation graphs, which reads the features from source nodes and writes the\n","    updated ones to destination nodes. If multiple relations have the same\n","    destination node types, their results are aggregated by the specified method.\n","    If the relation graph has no edge, the corresponding module will not be called.\n","\n","    Pseudo-code:\n","\n","    .. code::\n","\n","        outputs = {nty : [] for nty in g.dsttypes}\n","        # Apply sub-modules on their associating relation graphs in parallel\n","        for relation in g.canonical_etypes:\n","            stype, etype, dtype = relation\n","            dstdata = relation_submodule(g[relation], ...)\n","            outputs[dtype].append(dstdata)\n","\n","        # Aggregate the results for each destination node type\n","        rsts = {}\n","        for ntype, ntype_outputs in outputs.items():\n","            if len(ntype_outputs) != 0:\n","                rsts[ntype] = aggregate(ntype_outputs)\n","        return rsts\n","\n","    Examples\n","    --------\n","\n","    Create a heterograph with three types of relations and nodes.\n","\n","    >>> import dgl\n","    >>> g = dgl.heterograph({\n","    ...     ('user', 'follows', 'user') : edges1,\n","    ...     ('user', 'plays', 'game') : edges2,\n","    ...     ('store', 'sells', 'game')  : edges3})\n","\n","    Create a ``HeteroGraphConv`` that applies different convolution modules to\n","    different relations. Note that the modules for ``'follows'`` and ``'plays'``\n","    do not share weights.\n","\n","    >>> import dgl.nn.pytorch as dglnn\n","    >>> conv = dglnn.HeteroGraphConv({\n","    ...     'follows' : dglnn.GraphConv(...),\n","    ...     'plays' : dglnn.GraphConv(...),\n","    ...     'sells' : dglnn.SAGEConv(...)},\n","    ...     aggregate='sum')\n","\n","    Call forward with some ``'user'`` features. This computes new features for both\n","    ``'user'`` and ``'game'`` nodes.\n","\n","    >>> import torch as th\n","    >>> h1 = {'user' : th.randn((g.num_nodes('user'), 5))}\n","    >>> h2 = conv(g, h1)\n","    >>> print(h2.keys())\n","    dict_keys(['user', 'game'])\n","\n","    Call forward with both ``'user'`` and ``'store'`` features. Because both the\n","    ``'plays'`` and ``'sells'`` relations will update the ``'game'`` features,\n","    their results are aggregated by the specified method (i.e., summation here).\n","\n","    >>> f1 = {'user' : ..., 'store' : ...}\n","    >>> f2 = conv(g, f1)\n","    >>> print(f2.keys())\n","    dict_keys(['user', 'game'])\n","\n","    Call forward with some ``'store'`` features. This only computes new features\n","    for ``'game'`` nodes.\n","\n","    >>> g1 = {'store' : ...}\n","    >>> g2 = conv(g, g1)\n","    >>> print(g2.keys())\n","    dict_keys(['game'])\n","\n","    Call forward with a pair of inputs is allowed and each submodule will also\n","    be invoked with a pair of inputs.\n","\n","    >>> x_src = {'user' : ..., 'store' : ...}\n","    >>> x_dst = {'user' : ..., 'game' : ...}\n","    >>> y_dst = conv(g, (x_src, x_dst))\n","    >>> print(y_dst.keys())\n","    dict_keys(['user', 'game'])\n","\n","    Parameters\n","    ----------\n","    mods : dict[str, nn.Module]\n","        Modules associated with every edge types. The forward function of each\n","        module must have a `DGLGraph` object as the first argument, and\n","        its second argument is either a tensor object representing the node\n","        features or a pair of tensor object representing the source and destination\n","        node features.\n","    aggregate : str, callable, optional\n","        Method for aggregating node features generated by different relations.\n","        Allowed string values are 'sum', 'max', 'min', 'mean', 'stack'.\n","        The 'stack' aggregation is performed along the second dimension, whose order\n","        is deterministic.\n","        User can also customize the aggregator by providing a callable instance.\n","        For example, aggregation by summation is equivalent to the follows:\n","\n","        .. code::\n","\n","            def my_agg_func(tensors, dsttype):\n","                # tensors: is a list of tensors to aggregate\n","                # dsttype: string name of the destination node type for which the\n","                #          aggregation is performed\n","                stacked = torch.stack(tensors, dim=0)\n","                return torch.sum(stacked, dim=0)\n","\n","    Attributes\n","    ----------\n","    mods : dict[str, nn.Module]\n","        Modules associated with every edge types.\n","    \"\"\"\n","\n","    def __init__(self, mods, aggregate=\"sum\"):\n","        super(HeteroGraphConv, self).__init__()\n","        self.mod_dict = mods\n","        mods = {str(k): v for k, v in mods.items()}\n","        # Register as child modules\n","        self.mods = nn.ModuleDict(mods)\n","        # PyTorch ModuleDict doesn't have get() method, so I have to store two\n","        # dictionaries so that I can index with both canonical edge type and\n","        # edge type with the get() method.\n","        # Do not break if graph has 0-in-degree nodes.\n","        # Because there is no general rule to add self-loop for heterograph.\n","        for _, v in self.mods.items():\n","            set_allow_zero_in_degree_fn = getattr(\n","                v, \"set_allow_zero_in_degree\", None\n","            )\n","            if callable(set_allow_zero_in_degree_fn):\n","                set_allow_zero_in_degree_fn(True)\n","        if isinstance(aggregate, str):\n","            self.agg_fn = get_aggregate_fn(aggregate)\n","        else:\n","            self.agg_fn = aggregate\n","\n","    def _get_module(self, etype):\n","        mod = self.mod_dict.get(etype, None)\n","        if mod is not None:\n","            return mod\n","        if isinstance(etype, tuple):\n","            # etype is canonical\n","            _, etype, _ = etype\n","            return self.mod_dict[etype]\n","        raise KeyError(\"Cannot find module with edge type %s\" % etype)\n","\n","    def forward(self, g, inputs, mod_args=None, mod_kwargs=None):\n","        \"\"\"Forward computation\n","\n","        Invoke the forward function with each module and aggregate their results.\n","\n","        Parameters\n","        ----------\n","        g : DGLGraph\n","            Graph data.\n","        inputs : dict[str, Tensor] or pair of dict[str, Tensor]\n","            Input node features.\n","        mod_args : dict[str, tuple[any]], optional\n","            Extra positional arguments for the sub-modules.\n","        mod_kwargs : dict[str, dict[str, any]], optional\n","            Extra key-word arguments for the sub-modules.\n","\n","        Returns\n","        -------\n","        dict[str, Tensor]\n","            Output representations for every types of nodes.\n","        \"\"\"\n","        if mod_args is None:\n","            mod_args = {}\n","        if mod_kwargs is None:\n","            mod_kwargs = {}\n","        outputs = {nty: [] for nty in g.dsttypes}\n","        if isinstance(inputs, tuple) or g.is_block:\n","            if isinstance(inputs, tuple):\n","                src_inputs, dst_inputs = inputs\n","            else:\n","                src_inputs = inputs\n","                dst_inputs = {\n","                    k: v[: g.number_of_dst_nodes(k)] for k, v in inputs.items()\n","                }\n","\n","            for stype, etype, dtype in g.canonical_etypes:\n","                rel_graph = g[stype, etype, dtype]\n","                if stype not in src_inputs or dtype not in dst_inputs:\n","                    continue\n","                dstdata = self._get_module((stype, etype, dtype))(\n","                    rel_graph,\n","                    (src_inputs[stype], dst_inputs[dtype]),\n","                    *mod_args.get(etype, ()),\n","                    **mod_kwargs.get(etype, {})\n","                )\n","                outputs[dtype].append(dstdata)\n","        else:\n","            for stype, etype, dtype in g.canonical_etypes:\n","                rel_graph = g[stype, etype, dtype]\n","                if stype not in inputs:\n","                    continue\n","                dstdata = self._get_module((stype, etype, dtype))(\n","                    rel_graph,\n","                    (inputs[stype], inputs[dtype]),\n","                    *mod_args.get(etype, ()),\n","                    **mod_kwargs.get(etype, {})\n","                )\n","                outputs[dtype].append(dstdata)\n","        rsts = {}\n","        for nty, alist in outputs.items():\n","            if len(alist) != 0:\n","                rsts[nty] = self.agg_fn(alist, nty)\n","        return rsts\n","\n","\n","\n","def _max_reduce_func(inputs, dim):\n","    return th.max(inputs, dim=dim)[0]\n","\n","\n","def _min_reduce_func(inputs, dim):\n","    return th.min(inputs, dim=dim)[0]\n","\n","\n","def _sum_reduce_func(inputs, dim):\n","    return th.sum(inputs, dim=dim)\n","\n","\n","def _mean_reduce_func(inputs, dim):\n","    return th.mean(inputs, dim=dim)\n","\n","\n","def _stack_agg_func(inputs, dsttype):  # pylint: disable=unused-argument\n","    if len(inputs) == 0:\n","        return None\n","    return th.stack(inputs, dim=1)\n","\n","\n","def _agg_func(inputs, dsttype, fn):  # pylint: disable=unused-argument\n","    if len(inputs) == 0:\n","        return None\n","    stacked = th.stack(inputs, dim=0)\n","    return fn(stacked, dim=0)\n","\n","\n","def get_aggregate_fn(agg):\n","    \"\"\"Internal function to get the aggregation function for node data\n","    generated from different relations.\n","\n","    Parameters\n","    ----------\n","    agg : str\n","        Method for aggregating node features generated by different relations.\n","        Allowed values are 'sum', 'max', 'min', 'mean', 'stack'.\n","\n","    Returns\n","    -------\n","    callable\n","        Aggregator function that takes a list of tensors to aggregate\n","        and returns one aggregated tensor.\n","    \"\"\"\n","    if agg == \"sum\":\n","        fn = _sum_reduce_func\n","    elif agg == \"max\":\n","        fn = _max_reduce_func\n","    elif agg == \"min\":\n","        fn = _min_reduce_func\n","    elif agg == \"mean\":\n","        fn = _mean_reduce_func\n","    elif agg == \"stack\":\n","        fn = None  # will not be called\n","    else:\n","        raise DGLError(\n","            \"Invalid cross type aggregator. Must be one of \"\n","            '\"sum\", \"max\", \"min\", \"mean\" or \"stack\". But got \"%s\"' % agg\n","        )\n","    if agg == \"stack\":\n","        return _stack_agg_func\n","    else:\n","        return partial(_agg_func, fn=fn)\n","\n","class HeteroLinear(nn.Module):\n","    \"\"\"Apply linear transformations on heterogeneous inputs.\n","\n","    Parameters\n","    ----------\n","    in_size : dict[key, int]\n","        Input feature size for heterogeneous inputs. A key can be a string or a tuple of strings.\n","    out_size : int\n","        Output feature size.\n","    bias : bool, optional\n","        If True, learns a bias term. Defaults: ``True``.\n","\n","    Examples\n","    --------\n","\n","    >>> import dgl\n","    >>> import torch\n","    >>> from dgl.nn import HeteroLinear\n","\n","    >>> layer = HeteroLinear({'user': 1, ('user', 'follows', 'user'): 2}, 3)\n","    >>> in_feats = {'user': torch.randn(2, 1), ('user', 'follows', 'user'): torch.randn(3, 2)}\n","    >>> out_feats = layer(in_feats)\n","    >>> print(out_feats['user'].shape)\n","    torch.Size([2, 3])\n","    >>> print(out_feats[('user', 'follows', 'user')].shape)\n","    torch.Size([3, 3])\n","    \"\"\"\n","\n","    def __init__(self, in_size, out_size, bias=True):\n","        super(HeteroLinear, self).__init__()\n","\n","        self.linears = nn.ModuleDict()\n","        for typ, typ_in_size in in_size.items():\n","            self.linears[str(typ)] = nn.Linear(typ_in_size, out_size, bias=bias)\n","\n","    def forward(self, feat):\n","        \"\"\"Forward function\n","\n","        Parameters\n","        ----------\n","        feat : dict[key, Tensor]\n","            Heterogeneous input features. It maps keys to features.\n","\n","        Returns\n","        -------\n","        dict[key, Tensor]\n","            Transformed features.\n","        \"\"\"\n","        out_feat = dict()\n","        for typ, typ_feat in feat.items():\n","            out_feat[typ] = self.linears[str(typ)](typ_feat)\n","\n","        return out_feat\n","\n","class HeteroEmbedding(nn.Module):\n","    \"\"\"Create a heterogeneous embedding table.\n","\n","    It internally contains multiple ``torch.nn.Embedding`` with different dictionary sizes.\n","\n","    Parameters\n","    ----------\n","    num_embeddings : dict[key, int]\n","        Size of the dictionaries. A key can be a string or a tuple of strings.\n","    embedding_dim : int\n","        Size of each embedding vector.\n","\n","    Examples\n","    --------\n","\n","    >>> import dgl\n","    >>> import torch\n","    >>> from dgl.nn import HeteroEmbedding\n","\n","    >>> layer = HeteroEmbedding({'user': 2, ('user', 'follows', 'user'): 3}, 4)\n","    >>> # Get the heterogeneous embedding table\n","    >>> embeds = layer.weight\n","    >>> print(embeds['user'].shape)\n","    torch.Size([2, 4])\n","    >>> print(embeds[('user', 'follows', 'user')].shape)\n","    torch.Size([3, 4])\n","\n","    >>> # Get the embeddings for a subset\n","    >>> input_ids = {'user': torch.LongTensor([0]),\n","    ...              ('user', 'follows', 'user'): torch.LongTensor([0, 2])}\n","    >>> embeds = layer(input_ids)\n","    >>> print(embeds['user'].shape)\n","    torch.Size([1, 4])\n","    >>> print(embeds[('user', 'follows', 'user')].shape)\n","    torch.Size([2, 4])\n","    \"\"\"\n","\n","    def __init__(self, num_embeddings, embedding_dim):\n","        super(HeteroEmbedding, self).__init__()\n","\n","        self.embeds = nn.ModuleDict()\n","        self.raw_keys = dict()\n","        for typ, typ_num_rows in num_embeddings.items():\n","            self.embeds[str(typ)] = nn.Embedding(typ_num_rows, embedding_dim)\n","            self.raw_keys[str(typ)] = typ\n","\n","    @property\n","    def weight(self):\n","        \"\"\"Get the heterogeneous embedding table\n","\n","        Returns\n","        -------\n","        dict[key, Tensor]\n","            Heterogeneous embedding table\n","        \"\"\"\n","        return {\n","            self.raw_keys[typ]: emb.weight for typ, emb in self.embeds.items()\n","        }\n","\n","    def reset_parameters(self):\n","        \"\"\"\n","        Use the xavier method in nn.init module to make the parameters uniformly distributed\n","        \"\"\"\n","        for typ in self.embeds.keys():\n","            nn.init.xavier_uniform_(self.embeds[typ].weight)\n","\n","    def forward(self, input_ids):\n","        \"\"\"Forward function\n","\n","        Parameters\n","        ----------\n","        input_ids : dict[key, Tensor]\n","            The row IDs to retrieve embeddings. It maps a key to key-specific IDs.\n","\n","        Returns\n","        -------\n","        dict[key, Tensor]\n","            The retrieved embeddings.\n","        \"\"\"\n","        embeds = dict()\n","        for typ, typ_ids in input_ids.items():\n","            embeds[typ] = self.embeds[str(typ)](typ_ids)\n","\n","        return embeds\n"],"metadata":{"id":"fbTTpIb2lxby","executionInfo":{"status":"ok","timestamp":1689277123276,"user_tz":-480,"elapsed":447,"user":{"displayName":"Dong Dong","userId":"17680164657657523368"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"i4rvfMRJrB92","executionInfo":{"status":"ok","timestamp":1689277123276,"user_tz":-480,"elapsed":2,"user":{"displayName":"Dong Dong","userId":"17680164657657523368"}}},"execution_count":15,"outputs":[]}]}