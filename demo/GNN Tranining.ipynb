{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","toc_visible":true,"authorship_tag":"ABX9TyN59eGBDK7lyA7cZNqn0S2x"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dpw2XKUS1NCj","executionInfo":{"status":"ok","timestamp":1685503262318,"user_tz":-480,"elapsed":727,"user":{"displayName":"Dong Dong","userId":"17680164657657523368"}},"outputId":"44684e48-7b56-4004-a2d4-b2a9655c9d3d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Wed May 31 03:21:01 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   50C    P8    12W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","source":["!pip install  dgl -q -f https://data.dgl.ai/wheels/cu118/repo.html\n","!pip install  dglgo -q -f https://data.dgl.ai/wheels-test/repo.html"],"metadata":{"id":"KAQvTrVx1MzL","executionInfo":{"status":"ok","timestamp":1685503313247,"user_tz":-480,"elapsed":50930,"user":{"displayName":"Dong Dong","userId":"17680164657657523368"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"ebe687bb-912a-48e1-9057-a938a4efd15a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.4/86.4 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.5/63.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.4/52.4 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.9/111.9 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.4/29.4 MB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m100.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.6/485.6 kB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m572.7/572.7 kB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"code","source":["!pip install -U -q wandb"],"metadata":{"id":"hk4RGyDBaiyl","executionInfo":{"status":"ok","timestamp":1685503324375,"user_tz":-480,"elapsed":11133,"user":{"displayName":"Dong Dong","userId":"17680164657657523368"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"f81804cf-a1ee-48fa-b8c8-aca85f7c521c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m206.5/206.5 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"code","source":["import os\n","os.environ[\"DGLBACKEND\"] = \"pytorch\"\n","\n","import dgl\n","import torch\n","import torch.nn.functional as F\n","import scipy.sparse as sp"],"metadata":{"id":"0XYLl2QO1Vl0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# GNN Training\n","\n","本章節將會進入到 GNN 的訓練環節，我們會使用以下 4 種任務帶大家認識 GNN 的訓練過程，並且，本章節使用 pytorch 作為後端框架\n","\n","1. Node Classification/Regression\n","2. Edge Classification/Regression\n","3. Link Prediction\n","4. Graph Classification"],"metadata":{"id":"4S5dL80N0b3k"}},{"cell_type":"markdown","source":["## 節點分類\n","\n","在節點分類的任務中，我們輸入一張圖，並對圖上的每個節點進行分類，所以我們不需要 `GraphDataLoader` 批次讀取圖資料，相反地，我們只要有圖的信息結構即可"],"metadata":{"id":"z202nEyW3kcx"}},{"cell_type":"markdown","source":["### Example Dataset\n","\n","在節點分類的例子中，我們使用 `CoraGraphDataset`，這是一個非常著名的節點分類圖，該資料蒐集了 2708 個機器學習論文 (nodes)，每篇論文來自七個不同的機器學習主題 (神經網絡、概率方法、遺傳算法、監督學習、無監督學習、強化學習和規則學習)，節點代表論文之間的引用關係，更具體的說，有以下特徵\n","\n","- 節點數量: 2708\n","- 邊的數量: 10556\n","- 節點特徵維度: 1433\n","- 類別數量: 7"],"metadata":{"id":"2uisw-cu3mzj"}},{"cell_type":"markdown","source":["### Training phase\n","\n","在訓練階段，我們首先對每個節點劃分到訓練、驗證、測試三個資料集中，在前向傳播階段 `forward(graph, inputs)`，我們需要輸入圖的結構信息 (方便做 message passing) 和節點特徵 (知道當前要預測哪個節點)"],"metadata":{"id":"SOsdZAMSUBIp"}},{"cell_type":"markdown","source":["### Testing phase\n","\n","同 training phase，只需輸入節點特徵與圖即可"],"metadata":{"id":"FjcBPzdm3pn-"}},{"cell_type":"code","source":["# utils.py\n","import random\n","import numpy as np\n","import torch\n","import dgl\n","import wandb\n","\n","def seed_everything(seed):\n","    # Set Python random seed\n","    random.seed(seed)\n","\n","    # Set NumPy random seed\n","    np.random.seed(seed)\n","\n","    # Set PyTorch random seed for CPU and GPU\n","    torch.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed_all(seed)\n","\n","    # Set PyTorch deterministic operations for cudnn backend\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","    # Set DGL random seed\n","    # dgl.seed(seed)\n","    dgl.random.seed(seed)\n","\n","def wandb_settings(key, config, project, entity, name):\n","\n","    # Wandb Login\n","    wandb.login(key=key)\n","\n","    # Initialize W&B\n","    run = wandb.init(\n","        config=config,\n","        project=project,\n","        entity=entity,\n","        name=name,\n","    )\n","\n","def get_id(mask):\n","  return torch.LongTensor(np.nonzero(mask)).squeeze()\n","# seed_everything(42)"],"metadata":{"id":"9dAoqYPOZ-4u"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WoNdzIoD0Vwj"},"outputs":[],"source":["# model.py\n","import dgl.nn as dglnn\n","import torch.nn as nn\n","import torch.nn.functional as F\n","class SAGE(nn.Module):\n","    def __init__(self, in_feats, hid_feats, out_feats):\n","        super().__init__()\n","        # 实例化SAGEConve，in_feats是输入特征的维度，out_feats是输出特征的维度，aggregator_type是聚合函数的类型\n","        self.conv1 = dglnn.SAGEConv(\n","            in_feats=in_feats, out_feats=hid_feats, aggregator_type='mean')\n","        self.conv2 = dglnn.SAGEConv(\n","            in_feats=hid_feats, out_feats=out_feats, aggregator_type='mean')\n","\n","    def forward(self, graph, inputs):\n","        # 输入是节点的特征\n","        h = self.conv1(graph, inputs)\n","        h = F.relu(h)\n","        h = self.conv2(graph, h)\n","        return h"]},{"cell_type":"code","source":["# main.py\n","import argparse\n","import sys\n","\n","sys.argv = ['main.py']\n","\n","def parse_args():\n","    parser = argparse.ArgumentParser(description='DGL GNN node classification')\n","    parser.add_argument('--epochs', type=int, default=200, help='number of training epochs')\n","    parser.add_argument(\"--print_freq\", type=int, default=10, help=\"print frequency\")\n","    parser.add_argument('--save_name', type=str, default='model.pth', help='name of the file to save the trained model')\n","    parser.add_argument('--lr', type=float, default=1e-2, help='learning rate for the optimizer')\n","    parser.add_argument('--weight_decay', type=float, default=0, help='weight decay for the optimizer')\n","    parser.add_argument('--resume', type=bool, default=False, help='resume or not')\n","    parser.add_argument('--resume_path', type=str, default=\"best_model.pth\", help='resume path')\n","    parser.add_argument('--num_classes', type=int, default=7, help='number of classes')\n","    parser.add_argument('--hidden_feats', type=int, default=100, help='number of hidden units')\n","    parser.add_argument('--model_name', type=str, default=\"tv_densenet121\", help='model name')\n","    parser.add_argument('--device', type=str, default=\"cuda\", help='device')\n","    parser.add_argument('--rank', type=int, default=0, help='the rank of the device, 0 if single GPU')\n","    parser.add_argument('--name', type=str, default=\"test\", help='name for the wandb run')\n","    args = parser.parse_args()\n","    return args\n","\n","def main():\n","\n","    seed_everything(42)\n","\n","    args = parse_args()\n","\n","    d = vars(args)\n","    key = \"Your API key\"\n","    wandb_settings(key, d, \"DGL-Cora-Node-Classification\", \"DDCVLAB\", args.name)\n","\n","    dataset = CoraGraphDataset()\n","    graph = dataset[0]\n","\n","    features = graph.ndata[\"feat\"]\n","    labels = graph.ndata[\"label\"]\n","\n","    train_mask = graph.ndata[\"train_mask\"]\n","    val_mask = graph.ndata[\"val_mask\"]\n","    test_mask = graph.ndata[\"test_mask\"]\n","\n","    model = SAGE(in_feats=features.shape[1], hid_feats=args.hidden_feats, out_feats=args.num_classes)\n","    if args.resume:\n","      state_dict = torch.load(args.resume_path)\n","      model.load_state_dict(state_dict, strict=True)\n","    if args.device == \"cuda\":\n","      device = f\"cuda:{args.rank}\"\n","      model.to(device)\n","      graph = graph.to(device)\n","      features = features.to(device)\n","      labels = labels.to(device)\n","      train_mask = train_mask.to(device)\n","      val_mask = val_mask.to(device)\n","      test_mask = test_mask.to(device)\n","\n","    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n","    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=5)\n","\n","    best_val_acc = 0\n","    best_test_acc = 0\n","\n","    for epoch in range(args.epochs):\n","        # Forward\n","        logits = model(graph, features)\n","\n","        # Compute prediction\n","        pred = logits.argmax(1)\n","\n","        # Compute loss\n","        # Note that you should only compute the losses of the nodes in the training set.\n","        train_loss = F.cross_entropy(logits[train_mask], labels[train_mask])\n","\n","        # Compute accuracy on training/validation/test\n","        train_acc = (pred[train_mask] == labels[train_mask]).float().mean()\n","\n","        # Backward\n","        optimizer.zero_grad()\n","        train_loss.backward()\n","        optimizer.step()\n","\n","        # Validation\n","        with torch.no_grad():\n","          valid_loss = F.cross_entropy(logits[val_mask], labels[val_mask])\n","          val_acc = (pred[val_mask] == labels[val_mask]).float().mean()\n","\n","        # Save the best validation accuracy and the corresponding test accuracy.\n","        if best_val_acc < val_acc:\n","            best_val_acc = val_acc\n","            torch.save(model.state_dict(), args.save_name)\n","\n","\n","        if epoch % args.print_freq == 0:\n","            print(\n","                \"Epoch [{}] | loss: {:.3f}, val acc: {:.3f} (best {:.3f})\".format(\n","                    epoch, valid_loss, val_acc, best_val_acc\n","                )\n","            )\n","\n","        wandb.log({\"Train ACC\": train_acc, \"Epoch\": epoch})\n","        wandb.log({\"Val ACC\": val_acc, \"Epoch\": epoch})\n","\n","        wandb.log({\"Train Loss\": train_loss, \"Epoch\": epoch})\n","        wandb.log({\"Val Loss\": valid_loss, \"Epoch\": epoch})\n","\n","        scheduler.step(valid_loss)\n","\n","    # Testing\n","    state_dict = torch.load(args.save_name)\n","    model.load_state_dict(state_dict)\n","    model.eval()\n","\n","    logits = model(graph, features)\n","    pred = logits.argmax(1)\n","    test_acc = (pred[test_mask] == labels[test_mask]).float().mean()\n","\n","    print(f\"Test ACC: {test_acc}\")\n","    wandb.log({\"Test ACC\": test_acc})\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":534},"id":"GAC0tP6qTQcR","executionInfo":{"status":"error","timestamp":1684842689378,"user_tz":-480,"elapsed":8558,"user":{"displayName":"Dong Dong","userId":"17680164657657523368"}},"outputId":"69b2f918-0ac2-4d8a-b0a6-46ff0c8bc4a0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mddcvlab\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.15.3"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20230523_115124-q086bnyo</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/ddcvlab/DGL-Cora-Node-Classification/runs/q086bnyo' target=\"_blank\">test</a></strong> to <a href='https://wandb.ai/ddcvlab/DGL-Cora-Node-Classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/ddcvlab/DGL-Cora-Node-Classification' target=\"_blank\">https://wandb.ai/ddcvlab/DGL-Cora-Node-Classification</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/ddcvlab/DGL-Cora-Node-Classification/runs/q086bnyo' target=\"_blank\">https://wandb.ai/ddcvlab/DGL-Cora-Node-Classification/runs/q086bnyo</a>"]},"metadata":{}},{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-a5e690af4dbe>\u001b[0m in \u001b[0;36m<cell line: 122>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-7-a5e690af4dbe>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mwandb_settings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DGL-Cora-Node-Classification\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DDCVLAB\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCoraGraphDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'CoraGraphDataset' is not defined"]}]},{"cell_type":"markdown","source":["### Node Classification on Heterograph\n","\n","接下來我們看看怎麼在異構圖上面做節點分類，我們首先定義異構圖"],"metadata":{"id":"XSHfLrsDvvAm"}},{"cell_type":"code","source":["import numpy as np\n","import torch\n","from sklearn.model_selection import train_test_split\n","\n","def build_heterograph():\n","  n_users = 1000\n","  n_items = 500\n","  n_follows = 3000\n","  n_clicks = 5000\n","  n_dislikes = 500\n","  n_hetero_features = 10\n","  n_user_classes = 5\n","  n_max_clicks = 10\n","\n","  follow_src = np.random.randint(0, n_users, n_follows)\n","  follow_dst = np.random.randint(0, n_users, n_follows)\n","  click_src = np.random.randint(0, n_users, n_clicks)\n","  click_dst = np.random.randint(0, n_items, n_clicks)\n","  dislike_src = np.random.randint(0, n_users, n_dislikes)\n","  dislike_dst = np.random.randint(0, n_items, n_dislikes)\n","\n","  hetero_graph = dgl.heterograph({\n","      ('user', 'follow', 'user'): (follow_src, follow_dst),\n","      ('user', 'followed-by', 'user'): (follow_dst, follow_src),\n","      ('user', 'click', 'item'): (click_src, click_dst),\n","      ('item', 'clicked-by', 'user'): (click_dst, click_src),\n","      ('user', 'dislike', 'item'): (dislike_src, dislike_dst),\n","      ('item', 'disliked-by', 'user'): (dislike_dst, dislike_src)})\n","\n","  hetero_graph.nodes['user'].data['feature'] = torch.randn(n_users, n_hetero_features)\n","  hetero_graph.nodes['item'].data['feature'] = torch.randn(n_items, n_hetero_features)\n","  hetero_graph.nodes['user'].data['label'] = torch.randint(0, n_user_classes, (n_users,))\n","  hetero_graph.edges['click'].data['label'] = torch.randint(1, n_max_clicks, (n_clicks,)).float()\n","  # 在user类型的节点和click类型的边上随机生成训练集的掩码\n","  train_mask, valid_mask, test_mask = gen_mask(n_users)\n","  hetero_graph.nodes['user'].data['train_mask'] = train_mask\n","  hetero_graph.nodes['user'].data['val_mask'] = valid_mask\n","  hetero_graph.nodes['user'].data['test_mask'] = test_mask\n","  train_mask, valid_mask, test_mask = gen_mask(n_clicks)\n","  hetero_graph.edges['click'].data['train_mask'] = train_mask\n","  hetero_graph.edges['click'].data['val_mask'] = valid_mask\n","  hetero_graph.edges['click'].data['test_mask'] = test_mask\n","  return hetero_graph\n","\n","def gen_mask(n):\n","  mask = torch.zeros(n, dtype=torch.bool)\n","  perm = torch.randperm(n)\n","  train_indices = perm[:int(0.6 * n)]\n","  val_indices = perm[int(0.6 * n):int(0.8 * n)]\n","  test_indices = perm[int(0.8 * n):]\n","  train_mask = torch.clone(mask)\n","  valid_mask = torch.clone(mask)\n","  test_mask = torch.clone(mask)\n","  train_mask[train_indices] = True\n","  valid_mask[val_indices] = True\n","  test_mask[test_indices] = True\n","  return train_mask, valid_mask, test_mask\n"],"metadata":{"id":"bF_-a3V5v5id"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define a Heterograph Conv model\n","\n","class RGCN(nn.Module):\n","    def __init__(self, in_feats, hid_feats, out_feats, rel_names):\n","        super().__init__()\n","        # 实例化HeteroGraphConv，in_feats是输入特征的维度，out_feats是输出特征的维度，aggregate是聚合函数的类型\n","        self.conv1 = dglnn.HeteroGraphConv({\n","            rel: dglnn.GraphConv(in_feats, hid_feats)\n","            for rel in rel_names}, aggregate='sum')\n","        self.conv2 = dglnn.HeteroGraphConv({\n","            rel: dglnn.GraphConv(hid_feats, out_feats)\n","            for rel in rel_names}, aggregate='sum')\n","\n","    def forward(self, graph, inputs):\n","        # 输入是节点的特征字典\n","        h = self.conv1(graph, inputs)\n","        h = {k: F.relu(v) for k, v in h.items()}\n","        h = self.conv2(graph, h)\n","        return h"],"metadata":{"id":"KPh-3GClv7sW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["對於異構圖的資料結構，我們使用字典的形式儲存，我們也會用一個字典儲存模型對於不同實體類別的輸出，並且在測試階段，我們可以只關注於特定實體上的分類成效"],"metadata":{"id":"kIrV72JW5YXC"}},{"cell_type":"code","source":["# main.py\n","import argparse\n","import sys\n","\n","sys.argv = ['main.py']\n","\n","def parse_args():\n","    parser = argparse.ArgumentParser(description='DGL GNN Heterograph node classification')\n","    parser.add_argument('--epochs', type=int, default=200, help='number of training epochs')\n","    parser.add_argument(\"--print_freq\", type=int, default=10, help=\"print frequency\")\n","    parser.add_argument('--save_name', type=str, default='model.pth', help='name of the file to save the trained model')\n","    parser.add_argument('--lr', type=float, default=1e-2, help='learning rate for the optimizer')\n","    parser.add_argument('--weight_decay', type=float, default=0, help='weight decay for the optimizer')\n","    parser.add_argument('--resume', type=bool, default=False, help='resume or not')\n","    parser.add_argument('--resume_path', type=str, default=\"best_model.pth\", help='resume path')\n","    parser.add_argument('--num_classes', type=int, default=7, help='number of classes')\n","    parser.add_argument('--hidden_feats', type=int, default=100, help='number of hidden units')\n","    parser.add_argument('--model_name', type=str, default=\"tv_densenet121\", help='model name')\n","    parser.add_argument('--device', type=str, default=\"cuda\", help='device')\n","    parser.add_argument('--rank', type=int, default=0, help='the rank of the device, 0 if single GPU')\n","    parser.add_argument('--name', type=str, default=\"test\", help='name for the wandb run')\n","    args = parser.parse_args()\n","    return args\n","\n","def main():\n","\n","    seed_everything(42)\n","\n","    args = parse_args()\n","\n","    d = vars(args)\n","    key = \"Your API Key\"\n","    wandb_settings(key, d, \"DGL-Cora-Heterograph-Node-Classification\", \"DDCVLAB\", args.name)\n","\n","    hetero_graph = build_heterograph()\n","\n","    user_feats = hetero_graph.nodes['user'].data['feature']\n","    item_feats = hetero_graph.nodes['item'].data['feature']\n","    features = {'user': user_feats, 'item': item_feats}\n","    labels = hetero_graph.nodes['user'].data['label']\n","\n","    train_mask = hetero_graph.nodes['user'].data['train_mask']\n","    val_mask = hetero_graph.nodes['user'].data['val_mask']\n","    test_mask = hetero_graph.nodes['user'].data['test_mask']\n","\n","    model = RGCN(10, 20, 5, hetero_graph.etypes)\n","    if args.resume:\n","      state_dict = torch.load(args.resume_path)\n","      model.load_state_dict(state_dict, strict=True)\n","\n","    if args.device==\"cuda\":\n","      device = f\"cuda:{args.rank}\"\n","    else:\n","      device = args.device\n","    model.to(device)\n","    hetero_graph = hetero_graph.to(device)\n","    user_feats = user_feats.to(device)\n","    item_feats = item_feats.to(device)\n","    labels = labels.to(device)\n","    features = {k: v.to(device=device, non_blocking=True) if hasattr(v, 'to') else v for k, v in features.items()}\n","    train_mask = train_mask.to(device)\n","    val_mask = val_mask.to(device)\n","    test_mask = test_mask.to(device)\n","\n","    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n","    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=5)\n","\n","    best_val_acc = 0\n","    best_test_acc = 0\n","\n","    for epoch in range(args.epochs):\n","        # Forward\n","        logits = model(hetero_graph, features)[\"user\"]\n","        # Compute prediction\n","        pred = logits.argmax(1)\n","\n","        # Compute loss\n","        # Note that you should only compute the losses of the nodes in the training set.\n","        train_loss = F.cross_entropy(logits[train_mask], labels[train_mask])\n","\n","        # Compute accuracy on training/validation/test\n","        train_acc = (pred[train_mask] == labels[train_mask]).float().mean()\n","\n","        # Backward\n","        optimizer.zero_grad()\n","        train_loss.backward()\n","        optimizer.step()\n","\n","        # Validation\n","        with torch.no_grad():\n","          valid_loss = F.cross_entropy(logits[val_mask], labels[val_mask])\n","          val_acc = (pred[val_mask] == labels[val_mask]).float().mean()\n","\n","        # Save the best validation accuracy and the corresponding test accuracy.\n","        if best_val_acc < val_acc:\n","            best_val_acc = val_acc\n","            torch.save(model.state_dict(), args.save_name)\n","\n","\n","        if epoch % args.print_freq == 0:\n","            print(\n","                \"Epoch [{}] | loss: {:.3f}, val acc: {:.3f} (best {:.3f})\".format(\n","                    epoch, valid_loss, val_acc, best_val_acc\n","                )\n","            )\n","\n","        wandb.log({\"Train ACC\": train_acc, \"Epoch\": epoch})\n","        wandb.log({\"Val ACC\": val_acc, \"Epoch\": epoch})\n","\n","        wandb.log({\"Train Loss\": train_loss, \"Epoch\": epoch})\n","        wandb.log({\"Val Loss\": valid_loss, \"Epoch\": epoch})\n","\n","        scheduler.step(valid_loss)\n","\n","    # Testing\n","    state_dict = torch.load(args.save_name)\n","    model.load_state_dict(state_dict)\n","    model.eval()\n","\n","    logits = model(hetero_graph, features)[\"user\"]\n","    pred = logits.argmax(1)\n","    test_acc = (pred[test_mask] == labels[test_mask]).float().mean()\n","\n","    print(f\"Test ACC: {test_acc}\")\n","    wandb.log({\"Test ACC\": test_acc})\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"FbtPwBhFwE4i","colab":{"base_uri":"https://localhost:8080/","height":878},"executionInfo":{"status":"error","timestamp":1685412519816,"user_tz":-480,"elapsed":7308,"user":{"displayName":"Dong Dong","userId":"17680164657657523368"}},"outputId":"75235783-78bb-4895-bcca-70b2ada3e579"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mddcvlab\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.15.3"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20230530_020836-vl1pw07a</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/ddcvlab/DGL-Cora-Heterograph-Node-Classification/runs/vl1pw07a' target=\"_blank\">test</a></strong> to <a href='https://wandb.ai/ddcvlab/DGL-Cora-Heterograph-Node-Classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/ddcvlab/DGL-Cora-Heterograph-Node-Classification' target=\"_blank\">https://wandb.ai/ddcvlab/DGL-Cora-Heterograph-Node-Classification</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/ddcvlab/DGL-Cora-Heterograph-Node-Classification/runs/vl1pw07a' target=\"_blank\">https://wandb.ai/ddcvlab/DGL-Cora-Heterograph-Node-Classification/runs/vl1pw07a</a>"]},"metadata":{}},{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-d737766fc2a1>\u001b[0m in \u001b[0;36m<cell line: 124>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-12-d737766fc2a1>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mwandb_settings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DGL-Cora-Heterograph-Node-Classification\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DDCVLAB\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mhetero_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_heterograph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0muser_feats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhetero_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'user'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'feature'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'build_heterograph' is not defined"]},{"output_type":"stream","name":"stdout","text":["Error in callback <function _WandbInit._pause_backend at 0x7f6302995120> (for post_run_cell):\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/backcall/backcall.py\u001b[0m in \u001b[0;36madapted\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;31m#            print(args, kwargs, unmatched_pos, cut_positional, unmatched_kw)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0madapted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36m_pause_backend\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m         \u001b[0;31m# Attempt to save the code on every execution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 413\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotebook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_ipynb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    414\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/jupyter.py\u001b[0m in \u001b[0;36msave_ipynb\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_ipynb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Problem saving notebook: {repr(e)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/jupyter.py\u001b[0m in \u001b[0;36m_save_ipynb\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0;31m# TODO: likely only save if the code has changed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mcolab_ipynb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattempt_colab_load_ipynb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcolab_ipynb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/jupyter.py\u001b[0m in \u001b[0;36mattempt_colab_load_ipynb\u001b[0;34m()\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcolab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;31m# This isn't thread safe, never call in a thread\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocking_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"get_ipynb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ipynb\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","source":["## 邊分類\n","\n","在邊分類的任務中，我們輸入一張圖，並對圖上的每個邊進行分類，所以我們不需要 `GraphDataLoader` 批次讀取圖資料，相反地，我們只要有圖的信息結構即可"],"metadata":{"id":"uY0coIKl3uAE"}},{"cell_type":"markdown","source":["### Example Dataset\n","\n","在邊分類的例子中，我們使用 `WN18Dataset`，這是一個非常著名的邊分類圖，該資料蒐集了廣泛的英語詞彙 (nodes)，總共有 18 種類別，約 40000 種實體"],"metadata":{"id":"VbUTtUFz6UIh"}},{"cell_type":"markdown","source":["### Training phase\n","\n","在訓練階段，我們首先對每個節點劃分到訓練、驗證、測試三個資料集中，在前向傳播階段 `forward(graph, inputs)`，我們需要輸入圖的結構信息 (方便做 message passing) 和節點特徵 (知道當前要預測哪個節點)"],"metadata":{"id":"GF7gw7bm3vtD"}},{"cell_type":"markdown","source":["### Testing phase\n","\n","同 training phase，只需輸入節點特徵與圖即可"],"metadata":{"id":"fYlfmM873w6o"}},{"cell_type":"code","source":["from dgl.data import WN18Dataset"],"metadata":{"id":"LqGOxuULwHHV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TwoLayerGCN(nn.Module):\n","    def __init__(self, in_dim, hid_dim, out_dim):\n","        \"\"\"两层的GCN模型\"\"\"\n","        super().__init__()\n","        self.conv1 = dglnn.GraphConv(in_dim, hid_dim, allow_zero_in_degree=True)\n","        self.conv2 = dglnn.GraphConv(hid_dim, out_dim, allow_zero_in_degree=True)\n","\n","    def forward(self, blocks, x):\n","        x = F.relu(self.conv1(blocks[0], x))\n","        x = F.relu(self.conv2(blocks[1], x))\n","        return x\n","\n","class Predictor(nn.Module):\n","    \"\"\"边预测模块，将边两端节点表示拼接，然后接一个线性变换，得到最后的分类表示输出\"\"\"\n","    def __init__(self, in_dim, num_classes):\n","        super().__init__()\n","        self.W = nn.Linear(2 * in_dim, num_classes)\n","\n","    def apply_edges(self, edges):\n","        data = torch.cat([edges.src['x'], edges.dst['x']], dim=-1)\n","        return {'score': self.W(data)}\n","\n","    def forward(self, edge_subgraph, x):\n","        with edge_subgraph.local_scope():\n","            edge_subgraph.ndata['x'] = x\n","            edge_subgraph.apply_edges(self.apply_edges)\n","            return edge_subgraph.edata['score']\n","\n","class MyModel(nn.Module):\n","    \"\"\"主模型：结构比较清晰\"\"\"\n","    def __init__(self, emb_dim, hid_dim, out_dim, num_classes, num_nodes):\n","        super().__init__()\n","        self.node_emb = nn.Embedding(num_nodes, emb_dim)\n","        self.gcn = TwoLayerGCN(emb_dim, hid_dim, out_dim)\n","        self.predictor = Predictor(out_dim, num_classes)\n","\n","    def forward(self, edge_subgraph, blocks, input_nodes):\n","        x = self.node_emb(input_nodes)\n","        x = self.gcn(blocks, x)\n","        return self.predictor(edge_subgraph, x)"],"metadata":{"id":"5alBucbnfWYE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# main.py\n","import argparse\n","import sys\n","from sklearn.metrics import accuracy_score\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","sys.argv = ['main.py']\n","\n","def parse_args():\n","    parser = argparse.ArgumentParser(description='DGL GNN Heterograph node classification')\n","    parser.add_argument('--epochs', type=int, default=50, help='number of training epochs')\n","    parser.add_argument('--batch_size', type=int, default=2048, help='batch size')\n","    parser.add_argument(\"--print_freq\", type=int, default=1, help=\"print frequency\")\n","    parser.add_argument('--save_name', type=str, default='model.pth', help='name of the file to save the trained model')\n","    parser.add_argument('--lr', type=float, default=1e-2, help='learning rate for the optimizer')\n","    parser.add_argument('--weight_decay', type=float, default=0, help='weight decay for the optimizer')\n","    parser.add_argument('--resume', type=bool, default=False, help='resume or not')\n","    parser.add_argument('--resume_path', type=str, default=\"best_model.pth\", help='resume path')\n","    parser.add_argument('--num_classes', type=int, default=7, help='number of classes')\n","    parser.add_argument('--hidden_feats', type=int, default=64, help='number of hidden units')\n","    parser.add_argument('--model_name', type=str, default=\"tv_densenet121\", help='model name')\n","    parser.add_argument('--device', type=str, default=\"cuda\", help='device')\n","    parser.add_argument('--rank', type=int, default=0, help='the rank of the device, 0 if single GPU')\n","    parser.add_argument('--name', type=str, default=\"test\", help='name for the wandb run')\n","    parser.add_argument('--fanout', type=list, default=[10,25], help='number of samples in training steps')\n","    args = parser.parse_args()\n","    return args\n","\n","def main():\n","\n","    seed_everything(42)\n","\n","    args = parse_args()\n","\n","    # d = vars(args)\n","    # key = \"Your API key\"\n","    # wandb_settings(key, d, \"DGL-Cora-Heterograph-Node-Classification\", \"DDCVLAB\", args.name)\n","\n","    dataset = WN18Dataset()\n","    graph = dataset[0]\n","\n","    train_mask = graph.edata['train_edge_mask']\n","    val_mask = graph.edata['valid_edge_mask']\n","    test_mask = graph.edata['test_edge_mask']\n","\n","    sampler = dgl.dataloading.NeighborSampler(args.fanout)\n","    sampler = dgl.dataloading.as_edge_prediction_sampler(sampler)\n","    train_dataloader = dgl.dataloading.DataLoader(\n","        graph,\n","        get_id(train_mask),\n","        sampler,\n","        batch_size=args.batch_size,\n","        shuffle=True,\n","        drop_last=False,\n","    )\n","    sampler = dgl.dataloading.MultiLayerFullNeighborSampler(2)\n","    sampler = dgl.dataloading.as_edge_prediction_sampler(sampler)\n","    valid_dataloader = dgl.dataloading.DataLoader(\n","        graph,\n","        get_id(val_mask),\n","        sampler,\n","        batch_size = args.batch_size\n","    )\n","    sampler = dgl.dataloading.MultiLayerFullNeighborSampler(2)\n","    sampler = dgl.dataloading.as_edge_prediction_sampler(sampler)\n","    test_dataloader = dgl.dataloading.DataLoader(\n","        graph,\n","        get_id(test_mask),\n","        sampler,\n","        batch_size = args.batch_size\n","    )\n","\n","\n","    model = MyModel(64, args.hidden_feats, args.hidden_feats, 18, graph.num_nodes())\n","    if args.resume:\n","      state_dict = torch.load(args.resume_path)\n","      model.load_state_dict(state_dict, strict=True)\n","\n","    device = args.device\n","    model.to(device)\n","\n","    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n","    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=5)\n","\n","    best_val_acc = 0\n","    best_test_acc = 0\n","    trn_label, trn_pred = [], []\n","    train_epoch_loss = 0\n","    valid_epoch_loss = 0\n","    for epoch in range(args.epochs):\n","        model.train()\n","        for step, (input_nodes, edges_subgraph, blocks) in enumerate(train_dataloader):\n","            # Forward\n","            edges_subgraph = edges_subgraph.to(device)\n","            blocks = [block.to(device) for block in blocks]\n","            input_nodes = input_nodes.to(device)\n","\n","            logits = model(edges_subgraph, blocks, input_nodes)\n","            # Compute prediction\n","            pred = logits.argmax(1)\n","\n","            # Compute loss\n","            # Note that you should only compute the losses of the nodes in the training set.\n","            train_loss = F.cross_entropy(logits, edges_subgraph.edata[\"etype\"])\n","            train_epoch_loss += train_loss.item()\n","\n","            # Compute accuracy on training/validation/test\n","            trn_label.extend(edges_subgraph.edata['etype'].cpu().numpy().tolist())\n","            trn_pred.extend(pred.detach().cpu().numpy().tolist())\n","\n","            # Backward\n","            optimizer.zero_grad()\n","            train_loss.backward()\n","            optimizer.step()\n","\n","        train_acc = accuracy_score(trn_label, trn_pred)\n","\n","        model.eval()\n","        with torch.no_grad():\n","            for step, (input_nodes, edges_subgraph, blocks) in enumerate(valid_dataloader):\n","                input_nodes = input_nodes.to(device)\n","                edges_subgraph = edges_subgraph.to(device)\n","                blocks = [block.to(device) for block in blocks]\n","\n","                # Validation\n","                logits = model(edges_subgraph, blocks, input_nodes)\n","                pred = logits.argmax(1)\n","                valid_loss = F.cross_entropy(logits, edges_subgraph.edata[\"etype\"])\n","                valid_epoch_loss += valid_loss.item()\n","                trn_label.extend(edges_subgraph.edata['etype'].cpu().numpy().tolist())\n","                trn_pred.extend(pred.detach().cpu().numpy().tolist())\n","\n","        val_acc = accuracy_score(trn_label, trn_pred)\n","\n","        # Save the best validation accuracy and the corresponding test accuracy.\n","        if best_val_acc < val_acc:\n","            best_val_acc = val_acc\n","            torch.save(model.state_dict(), args.save_name)\n","        if epoch % args.print_freq == 0:\n","            print(\n","                \"Epoch [{}] | loss: {:.3f}, val acc: {:.3f} (best {:.3f})\".format(\n","                    epoch, valid_loss, val_acc, best_val_acc\n","                )\n","            )\n","        # wandb.log({\"Train ACC\": train_acc, \"Epoch\": epoch})\n","        # wandb.log({\"Val ACC\": val_acc, \"Epoch\": epoch})\n","        # wandb.log({\"Train Loss\": train_loss, \"Epoch\": epoch})\n","        # wandb.log({\"Val Loss\": valid_loss, \"Epoch\": epoch})\n","\n","        scheduler.step(valid_loss)\n","\n","    # Testing\n","    state_dict = torch.load(args.save_name)\n","    model.load_state_dict(state_dict)\n","    model.eval()\n","\n","    trn_label, trn_pred = [], []\n","    with torch.no_grad():\n","        for step, (input_nodes, edges_subgraph, blocks) in enumerate(test_dataloader):\n","            input_nodes = input_nodes.to(device)\n","            edges_subgraph = edges_subgraph.to(device)\n","            blocks = [block.to(device) for block in blocks]\n","            logits = model(edges_subgraph, blocks, input_nodes)\n","            pred = logits.argmax(1)\n","            trn_label.extend(edges_subgraph.edata['etype'].cpu().numpy().tolist())\n","            trn_pred.extend(pred.detach().cpu().numpy().tolist())\n","\n","    test_acc = accuracy_score(trn_label, trn_pred)\n","\n","    print(f\"Test ACC: {test_acc}\")\n","    # wandb.log({\"Test ACC\": test_acc})\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"RNQN_BDofjpe","colab":{"base_uri":"https://localhost:8080/"},"outputId":"fd7e0b5b-4596-40b3-bc3c-4c5adeb1fbf1","executionInfo":{"status":"ok","timestamp":1685415760989,"user_tz":-480,"elapsed":268086,"user":{"displayName":"Dong Dong","userId":"17680164657657523368"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["# entities: 40943\n","# relations: 18\n","# training edges: 141442\n","# validation edges: 5000\n","# testing edges: 5000\n","Done loading data from cached files.\n","Epoch [0] | loss: 1.312, val acc: 0.507 (best 0.507)\n","Epoch [1] | loss: 0.962, val acc: 0.598 (best 0.598)\n","Epoch [2] | loss: 0.989, val acc: 0.651 (best 0.651)\n","Epoch [3] | loss: 0.991, val acc: 0.685 (best 0.685)\n","Epoch [4] | loss: 0.928, val acc: 0.711 (best 0.711)\n","Epoch [5] | loss: 0.720, val acc: 0.731 (best 0.731)\n","Epoch [6] | loss: 0.781, val acc: 0.748 (best 0.748)\n","Epoch [7] | loss: 0.759, val acc: 0.762 (best 0.762)\n","Epoch [8] | loss: 0.582, val acc: 0.774 (best 0.774)\n","Epoch [9] | loss: 0.715, val acc: 0.784 (best 0.784)\n","Epoch [10] | loss: 0.697, val acc: 0.794 (best 0.794)\n","Epoch [11] | loss: 0.637, val acc: 0.802 (best 0.802)\n","Epoch [12] | loss: 0.622, val acc: 0.809 (best 0.809)\n","Epoch [13] | loss: 0.585, val acc: 0.816 (best 0.816)\n","Epoch [14] | loss: 0.755, val acc: 0.822 (best 0.822)\n","Epoch [15] | loss: 0.545, val acc: 0.829 (best 0.829)\n","Epoch [16] | loss: 0.544, val acc: 0.835 (best 0.835)\n","Epoch [17] | loss: 0.537, val acc: 0.840 (best 0.840)\n","Epoch [18] | loss: 0.514, val acc: 0.845 (best 0.845)\n","Epoch [19] | loss: 0.526, val acc: 0.850 (best 0.850)\n","Epoch [20] | loss: 0.561, val acc: 0.854 (best 0.854)\n","Epoch [21] | loss: 0.486, val acc: 0.858 (best 0.858)\n","Epoch [22] | loss: 0.484, val acc: 0.861 (best 0.861)\n","Epoch [23] | loss: 0.492, val acc: 0.865 (best 0.865)\n","Epoch [24] | loss: 0.495, val acc: 0.868 (best 0.868)\n","Epoch [25] | loss: 0.497, val acc: 0.870 (best 0.870)\n","Epoch [26] | loss: 0.486, val acc: 0.873 (best 0.873)\n","Epoch [27] | loss: 0.499, val acc: 0.876 (best 0.876)\n","Epoch [28] | loss: 0.472, val acc: 0.878 (best 0.878)\n","Epoch [29] | loss: 0.480, val acc: 0.880 (best 0.880)\n","Epoch [30] | loss: 0.471, val acc: 0.882 (best 0.882)\n","Epoch [31] | loss: 0.473, val acc: 0.884 (best 0.884)\n","Epoch [32] | loss: 0.466, val acc: 0.886 (best 0.886)\n","Epoch [33] | loss: 0.462, val acc: 0.888 (best 0.888)\n","Epoch [34] | loss: 0.448, val acc: 0.889 (best 0.889)\n","Epoch [35] | loss: 0.473, val acc: 0.891 (best 0.891)\n","Epoch [36] | loss: 0.446, val acc: 0.892 (best 0.892)\n","Epoch [37] | loss: 0.479, val acc: 0.894 (best 0.894)\n","Epoch [38] | loss: 0.473, val acc: 0.895 (best 0.895)\n","Epoch [39] | loss: 0.454, val acc: 0.896 (best 0.896)\n","Epoch [40] | loss: 0.484, val acc: 0.898 (best 0.898)\n","Epoch [41] | loss: 0.482, val acc: 0.899 (best 0.899)\n","Epoch [42] | loss: 0.436, val acc: 0.900 (best 0.900)\n","Epoch [43] | loss: 0.431, val acc: 0.901 (best 0.901)\n","Epoch [44] | loss: 0.446, val acc: 0.902 (best 0.902)\n","Epoch [45] | loss: 0.432, val acc: 0.903 (best 0.903)\n","Epoch [46] | loss: 0.425, val acc: 0.904 (best 0.904)\n","Epoch [47] | loss: 0.447, val acc: 0.905 (best 0.905)\n","Epoch [48] | loss: 0.459, val acc: 0.906 (best 0.906)\n","Epoch [49] | loss: 0.453, val acc: 0.907 (best 0.907)\n","Test ACC: 0.8918\n"]}]},{"cell_type":"markdown","source":["## 鏈接預測 (Link Prediction)\n","\n","在鏈接預測的任務中，我們輸入一張圖，並對圖上的每兩個點進行預測，所以我們不需要 `GraphDataLoader` 批次讀取圖資料，相反地，我們只要有圖的信息結構即可\n","\n","Note: 我們以往的 sampler 都是在圖上選擇有關係的子圖，但是在鏈接預測的任務中，我們必須在訓練階段讓模型除了學習有鏈接的邊以外，也要學習無鏈接的邊\n","\n","用數學的方式來講，如果一對點 $(u, v)$ 形成一條邊，那個這對點的得分應該要比以 $u$ 為 src，任選一個 $v'$ 作為 dst 的分數要來的高。從一個機率分配 $v'\\sim P_v$ 中抽出 $v'$ 的過程稱為負採樣\n","\n","- `dgl.dataloading.negative_sampler.PerSourceUniform(k)`\n","   - `k:` 每條邊的負樣本個數\n","\n"],"metadata":{"id":"GFiwSwS-o_ZV"}},{"cell_type":"code","source":["class PerSourceUniform(dgl.dataloading.negative_sampler._BaseNegativeSampler):\n","    \"\"\"Negative sampler that randomly chooses negative destination nodes\n","    for each source node according to a uniform distribution.\n","\n","    For each edge ``(u, v)`` of type ``(srctype, etype, dsttype)``, DGL generates\n","    :attr:`k` pairs of negative edges ``(u, v')``, where ``v'`` is chosen\n","    uniformly from all the nodes of type ``dsttype``.  The resulting edges will\n","    also have type ``(srctype, etype, dsttype)``.\n","\n","    Parameters\n","    ----------\n","    k : int\n","        The number of negative samples per edge.\n","\n","    Examples\n","    --------\n","    >>> g = dgl.graph(([0, 1, 2], [1, 2, 3]))\n","    >>> neg_sampler = dgl.dataloading.negative_sampler.PerSourceUniform(2)\n","    >>> neg_sampler(g, torch.tensor([0, 1]))\n","    (tensor([0, 0, 1, 1]), tensor([1, 0, 2, 3]))\n","    \"\"\"\n","\n","    def __init__(self, k):\n","        self.k = k\n","\n","    def _generate(self, g, eids, canonical_etype):\n","        # dst 的型態 (dsttype)\n","        _, _, vtype = canonical_etype\n","        # 要抽多少個邊\n","        shape = F.shape(eids)\n","        # 邊的型態 (etype)\n","        dtype = F.dtype(eids)\n","        ctx = F.context(eids)\n","        shape = (shape[0] * self.k,)\n","        # find src\n","        src, _ = g.find_edges(eids, etype=canonical_etype)\n","        # sample exactly k negative sampler per src\n","        src = F.repeat(src, self.k, 0)\n","        # random select dst\n","        dst = F.randint(shape, dtype, ctx, 0, g.num_nodes(vtype))\n","        return src, dst"],"metadata":{"id":"yKB2yF_DfpJU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["g = dgl.graph(([0, 1, 2, 3, 4, 0, 1, 2, 3], [1, 2, 3, 4, 4, 4, 4, 4, 4]))\n","neg_sampler = dgl.dataloading.negative_sampler.PerSourceUniform(6)\n","neg_sampler(g, torch.tensor([0, 1]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dILwJMakqvbg","executionInfo":{"status":"ok","timestamp":1685418758521,"user_tz":-480,"elapsed":2,"user":{"displayName":"Dong Dong","userId":"17680164657657523368"}},"outputId":"f9183e56-e617-4c07-a57e-de0cab446405"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]),\n"," tensor([3, 4, 4, 0, 0, 1, 0, 0, 3, 3, 0, 2]))"]},"metadata":{},"execution_count":47}]},{"cell_type":"markdown","source":["### Custom Negative Sampler\n","\n","若要自訂 `Sampler`，我們必須繼承於 `dgl.dataloading.negative_sampler._BaseNegativeSampler`，並且實現 `_generate` 方法，該方法會在 `_BaseNegativeSampler` 中被調用，或者自訂義類別，且需要實現 `__call__` 魔術方法，以下範例我們用每個點的 in_degrees 來影響抽樣機率"],"metadata":{"id":"ZuYtBu1FyaLv"}},{"cell_type":"code","source":["class CustomSampler:\n","\n","    def __init__(self, graph, k, alpha=0.75):\n","      self._g = graph\n","      self.weights = self._g.in_degrees().float() ** alpha\n","      self.k = k\n","\n","    def __call__(self, graph, eids):\n","      src, _ = graph.find_edges(eids)\n","      # It's ok to use src.repeat(self.k)\n","      src = src.repeat_interleave(self.k)\n","      dst = self.weights.multinomial(len(src), replacement=True)\n","      return src, dst"],"metadata":{"id":"uEfzy5QqqyS-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["neg_sampler = CustomSampler(g, 6)\n","neg_sampler(g, torch.tensor([0, 4]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wHjPYKhFxQjg","executionInfo":{"status":"ok","timestamp":1685419424294,"user_tz":-480,"elapsed":4,"user":{"displayName":"Dong Dong","userId":"17680164657657523368"}},"outputId":"4d2be9fe-e0ec-4912-80f2-dda385bb5d22"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4]),\n"," tensor([4, 4, 4, 1, 4, 4, 3, 1, 4, 4, 4, 4]))"]},"metadata":{},"execution_count":67}]},{"cell_type":"code","source":["class CustomSampler(dgl.dataloading.negative_sampler._BaseNegativeSampler):\n","\n","    def __init__(self, k, alpha=0.75):\n","      self.alpha = alpha\n","      self.k = k\n","\n","    def _generate(self, graph, eids, canonical_etype):\n","      src, _ = graph.find_edges(eids)\n","      weights = graph.in_degrees().float() ** self.alpha\n","      src = src.repeat_interleave(self.k)\n","      dst = weights.multinomial(len(src), replacement=True)\n","      return src, dst"],"metadata":{"id":"gtoEo0zTxcMf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["neg_sampler = CustomSampler(6)\n","neg_sampler(g, torch.tensor([0, 4]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZYW2CGGnzZ86","executionInfo":{"status":"ok","timestamp":1685419204368,"user_tz":-480,"elapsed":10,"user":{"displayName":"Dong Dong","userId":"17680164657657523368"}},"outputId":"20c2b503-0f24-4baf-fe26-16493312c61f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4]),\n"," tensor([2, 4, 4, 3, 4, 4, 4, 4, 4, 4, 4, 1]))"]},"metadata":{},"execution_count":64}]},{"cell_type":"markdown","source":["### Negative Sampler DataLoader\n","\n","由於我們的任務是對邊進行預測，所以我們必須先指定 `sampler` 在通過 `as_edge_prediction_sampler` 轉換成對邊抽樣的同時指定 `negative_sampler`"],"metadata":{"id":"ItoFRnGD2QWy"}},{"cell_type":"code","source":["neg_sampler = CustomSampler(6)\n","sampler = dgl.dataloading.MultiLayerFullNeighborSampler(2)\n","sampler = dgl.dataloading.as_edge_prediction_sampler(sampler, negative_sampler=neg_sampler)\n","dataloader = dgl.dataloading.DataLoader(\n","    g,\n","    [i for i in range(g.num_edges())],\n","    sampler,\n","    batch_size=4,\n","    shuffle=False,\n","    drop_last=False\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RPxdS1JUza3r","executionInfo":{"status":"ok","timestamp":1685419903327,"user_tz":-480,"elapsed":3,"user":{"displayName":"Dong Dong","userId":"17680164657657523368"}},"outputId":"e811a693-feb8-4b7b-deff-9980ee4754db"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[tensor([0, 1, 2, 3, 4]),\n"," Graph(num_nodes=5, num_edges=4,\n","       ndata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64)}\n","       edata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64)}),\n"," Graph(num_nodes=5, num_edges=24,\n","       ndata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64)}\n","       edata_schemes={}),\n"," [Block(num_src_nodes=5, num_dst_nodes=5, num_edges=9),\n","  Block(num_src_nodes=5, num_dst_nodes=5, num_edges=9)]]"]},"metadata":{},"execution_count":80}]},{"cell_type":"markdown","source":["\n","#### Unpacking\n","\n","dataloader 是個 Iterator，回傳 `(input_nodes, pos_pair_graph, neg_pair_graph, blocks)` 的元組\n","\n","- `input_nodes:` 輸入節點的 ID，數量同 `blocks[0].number_of_src_nodes()`\n","- `pos_pair_graph:` 抽樣邊的正樣本子圖 (edge subgraph) [num_edges=batch_size]\n","- `neg_pair_graph:` 抽樣邊的負樣本子圖 (edge subgraph) [num_edges=batch_size*k]\n","- `blocks:` 列表，`blocks[i]` 表示 GNN 中 $i$-layer 至 $(i+1)$-layer 的二分子圖"],"metadata":{"id":"EoSgsCw_21Jp"}},{"cell_type":"code","source":["next(iter(dataloader))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"br4J98jO0rA7","executionInfo":{"status":"ok","timestamp":1685420115984,"user_tz":-480,"elapsed":628,"user":{"displayName":"Dong Dong","userId":"17680164657657523368"}},"outputId":"fff36f69-7ea9-4037-af5b-74ff045ad14a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[tensor([0, 1, 2, 3, 4]),\n"," Graph(num_nodes=5, num_edges=4,\n","       ndata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64)}\n","       edata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64)}),\n"," Graph(num_nodes=5, num_edges=24,\n","       ndata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64)}\n","       edata_schemes={}),\n"," [Block(num_src_nodes=5, num_dst_nodes=5, num_edges=9),\n","  Block(num_src_nodes=5, num_dst_nodes=5, num_edges=9)]]"]},"metadata":{},"execution_count":81}]},{"cell_type":"markdown","source":["### Example Dataset\n","\n","我們鏈接預測一樣使用 `CoraGraphDataset`，這是一個非常著名的節點分類圖，該資料蒐集了 2708 個機器學習論文 (nodes)，每篇論文來自七個不同的機器學習主題 (神經網絡、概率方法、遺傳算法、監督學習、無監督學習、強化學習和規則學習)，節點代表論文之間的引用關係，更具體的說，有以下特徵\n","\n","- 節點數量: 2708\n","- 邊的數量: 10556\n","- 節點特徵維度: 1433\n","- 類別數量: 7"],"metadata":{"id":"4q9kYBtJYnrn"}},{"cell_type":"code","source":["import dgl.function as fn\n","\n","dataset = dgl.data.CoraGraphDataset()\n","g = dataset[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B4zoHcH1Y3U7","executionInfo":{"status":"ok","timestamp":1685500372927,"user_tz":-480,"elapsed":1115,"user":{"displayName":"Dong Dong","userId":"17680164657657523368"}},"outputId":"ae7dbc06-e466-4522-b18c-733cdb324864"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["  NumNodes: 2708\n","  NumEdges: 10556\n","  NumFeats: 1433\n","  NumClasses: 7\n","  NumTrainingSamples: 140\n","  NumValidationSamples: 500\n","  NumTestSamples: 1000\n","Done loading data from cached files.\n"]}]},{"cell_type":"code","source":["# utils.py\n","def _get_mask(n, ids):\n","  zeros = torch.zeros(n)\n","  zeros[ids] = 1\n","  return zeros\n","\n","def _generate_edge_mask(graph, train_size=0.8, valid_size=0.1, test_size=0.1):\n","  src, dst = graph.edges()\n","  eids = np.arange(graph.num_edges())\n","  eids = np.random.permutation(eids)\n","\n","  overall_size = train_size + valid_size + test_size\n","  train_size = train_size / overall_size\n","  valid_size = valid_size / overall_size\n","  test_size = test_size / overall_size\n","\n","  train_size = int(len(eids) * train_size)\n","  valid_size = int(len(eids) * valid_size)\n","  test_size = int(len(eids) * test_size)\n","\n","  train_eids = eids[:train_size]\n","  valid_eids = eids[train_size:(train_size+valid_size)]\n","  test_eids = eids[(train_size+valid_size):]\n","\n","  graph.edata[\"train_edge_mask\"] = _get_mask(graph.num_edges(), train_eids)\n","  graph.edata[\"valid_edge_mask\"] = _get_mask(graph.num_edges(), valid_eids)\n","  graph.edata[\"test_edge_mask\"] = _get_mask(graph.num_edges(), test_eids)\n","\n","  return graph\n","\n","  # train_src, train_dst = src[eids[:train_size]], dst[eids[:train_size]]\n","  # valid_src, valid_dst = src[eids[train_size:(train_size+valid_size)]], dst[eids[train_size:(train_size+valid_size)]]\n","  # test_src, test_dst = src[eids[(train_size+valid_size):(train_size+valid_size+test_size)]], dst[eids[(train_size+valid_size):(train_size+valid_size+test_size)]]\n","\n","  # train_g = dgl.graph((train_src, train_dst), num_nodes=graph.num_nodes())\n","  # valid_g = dgl.graph((valid_src, valid_dst), num_nodes=graph.num_nodes())\n","  # test_g = dgl.graph((test_src, test_dst), num_nodes=graph.num_nodes())\n","\n","  # return train_g, valid_g, test_g"],"metadata":{"id":"B8nC4db0dWp1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class StochasticTwoLayerGCN(nn.Module):\n","    def __init__(self, in_dim, hid_dim, out_dim):\n","        \"\"\"两层的GCN模型\"\"\"\n","        super().__init__()\n","        self.conv1 = dglnn.GraphConv(in_dim, hid_dim, allow_zero_in_degree=True)\n","        self.conv2 = dglnn.GraphConv(hid_dim, out_dim, allow_zero_in_degree=True)\n","\n","    def forward(self, blocks, x):\n","        x = F.relu(self.conv1(blocks[0], x))\n","        x = F.relu(self.conv2(blocks[1], x))\n","        return x\n","\n","class DotPredictor(nn.Module):\n","    def forward(self, g, h):\n","        with g.local_scope():\n","            g.ndata[\"h\"] = h\n","            # Compute a new edge feature named 'score' by a dot-product between the\n","            # source node feature 'h' and destination node feature 'h'.\n","            g.apply_edges(fn.u_dot_v(\"h\", \"h\", \"score\"))\n","            # u_dot_v returns a 1-element vector for each edge so you need to squeeze it.\n","            return g.edata[\"score\"][:, 0]\n","\n","class MLPPredictor(nn.Module):\n","    def __init__(self, h_feats):\n","        super().__init__()\n","        self.W1 = nn.Linear(h_feats * 2, h_feats)\n","        self.W2 = nn.Linear(h_feats, 1)\n","\n","    def apply_edges(self, edges):\n","        \"\"\"\n","        Computes a scalar score for each edge of the given graph.\n","\n","        Parameters\n","        ----------\n","        edges :\n","            Has three members ``src``, ``dst`` and ``data``, each of\n","            which is a dictionary representing the features of the\n","            source nodes, the destination nodes, and the edges\n","            themselves.\n","\n","        Returns\n","        -------\n","        dict\n","            A dictionary of new edge features.\n","        \"\"\"\n","        h = torch.cat([edges.src[\"h\"], edges.dst[\"h\"]], 1)\n","        return {\"score\": self.W2(F.relu(self.W1(h))).squeeze(1)}\n","\n","    def forward(self, g, h):\n","        with g.local_scope():\n","            g.ndata[\"h\"] = h\n","            g.apply_edges(self.apply_edges)\n","            return g.edata[\"score\"]\n","\n","class Model(nn.Module):\n","    def __init__(self, in_features, hidden_features, out_features):\n","      super().__init__()\n","      self.gcn = StochasticTwoLayerGCN(in_features, hidden_features, out_features)\n","      self.predictor = DotPredictor()\n","\n","    def forward(self, input_nodes, pos_pair_graph, neg_pair_graph, blocks):\n","        input_nodes = self.gcn(blocks, input_nodes)\n","        pos_score = self.predictor(pos_pair_graph, input_nodes)\n","        neg_score = self.predictor(neg_pair_graph, input_nodes)\n","        return pos_score, neg_score\n","\n","def compute_loss(pos_score, neg_score):\n","    n = pos_score.shape[0]\n","    return (neg_score.view(n, -1) - pos_score.view(n, -1) + 1).clamp(min=0).mean()\n","\n","def compute_auc(pos_score, neg_score):\n","    pos_score = torch.tensor(pos_score)\n","    neg_score = torch.tensor(neg_score)\n","    scores = torch.cat([pos_score, neg_score]).numpy()\n","    labels = torch.cat(\n","        [torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])]\n","    ).numpy()\n","    return roc_auc_score(labels, scores)"],"metadata":{"id":"btoKpDSclMW8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# main.py\n","import argparse\n","import sys\n","from sklearn.metrics import roc_auc_score\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","sys.argv = ['main.py']\n","\n","def parse_args():\n","    parser = argparse.ArgumentParser(description='DGL GNN Heterograph node classification')\n","    parser.add_argument('--epochs', type=int, default=50, help='number of training epochs')\n","    parser.add_argument('--batch_size', type=int, default=512, help='batch size')\n","    parser.add_argument('--num_negatives', type=int, default=6, help='number of negative per edge')\n","    parser.add_argument(\"--print_freq\", type=int, default=1, help=\"print frequency\")\n","    parser.add_argument('--save_name', type=str, default='model.pth', help='name of the file to save the trained model')\n","    parser.add_argument('--lr', type=float, default=1e-2, help='learning rate for the optimizer')\n","    parser.add_argument('--weight_decay', type=float, default=0, help='weight decay for the optimizer')\n","    parser.add_argument('--resume', type=bool, default=False, help='resume or not')\n","    parser.add_argument('--resume_path', type=str, default=\"best_model.pth\", help='resume path')\n","    parser.add_argument('--num_classes', type=int, default=7, help='number of classes')\n","    parser.add_argument('--hidden_feats', type=int, default=16, help='number of hidden units')\n","    parser.add_argument('--model_name', type=str, default=\"tv_densenet121\", help='model name')\n","    parser.add_argument('--device', type=str, default=\"cuda\", help='device')\n","    parser.add_argument('--rank', type=int, default=0, help='the rank of the device, 0 if single GPU')\n","    parser.add_argument('--name', type=str, default=\"test\", help='name for the wandb run')\n","    parser.add_argument('--fanout', type=list, default=[10,25], help='number of samples in training steps')\n","    args = parser.parse_args()\n","    return args\n","\n","def main():\n","\n","    seed_everything(42)\n","\n","    args = parse_args()\n","\n","    # d = vars(args)\n","    # key = \"Your API key\"\n","    # wandb_settings(key, d, \"DGL-Cora-Heterograph-Node-Classification\", \"DDCVLAB\", args.name)\n","\n","    dataset = dgl.data.CoraGraphDataset()\n","    graph = dataset[0]\n","    graph = _generate_edge_mask(graph)\n","\n","    train_mask = graph.edata['train_edge_mask']\n","    val_mask = graph.edata['valid_edge_mask']\n","    test_mask = graph.edata['test_edge_mask']\n","\n","    sampler = dgl.dataloading.NeighborSampler(args.fanout)\n","    neg_sampler = dgl.dataloading.negative_sampler.PerSourceUniform(args.num_negatives)\n","    sampler = dgl.dataloading.as_edge_prediction_sampler(sampler, negative_sampler=neg_sampler)\n","    train_dataloader = dgl.dataloading.DataLoader(\n","        graph,\n","        get_id(train_mask),\n","        sampler,\n","        batch_size=args.batch_size,\n","        shuffle=True,\n","        drop_last=False,\n","    )\n","    sampler = dgl.dataloading.MultiLayerFullNeighborSampler(2)\n","    neg_sampler = dgl.dataloading.negative_sampler.PerSourceUniform(args.num_negatives)\n","    sampler = dgl.dataloading.as_edge_prediction_sampler(sampler, negative_sampler=neg_sampler)\n","    valid_dataloader = dgl.dataloading.DataLoader(\n","        graph,\n","        get_id(val_mask),\n","        sampler,\n","        batch_size = args.batch_size\n","    )\n","    sampler = dgl.dataloading.MultiLayerFullNeighborSampler(2)\n","    neg_sampler = dgl.dataloading.negative_sampler.PerSourceUniform(args.num_negatives)\n","    sampler = dgl.dataloading.as_edge_prediction_sampler(sampler, negative_sampler=neg_sampler)\n","    test_dataloader = dgl.dataloading.DataLoader(\n","        graph,\n","        get_id(test_mask),\n","        sampler,\n","        batch_size = args.batch_size\n","    )\n","\n","\n","    model = Model(graph.ndata[\"feat\"].shape[1], args.hidden_feats, args.hidden_feats)\n","    if args.resume:\n","      state_dict = torch.load(args.resume_path)\n","      model.load_state_dict(state_dict, strict=True)\n","\n","    device = args.device\n","    model.to(device)\n","\n","    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n","    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=5)\n","\n","    best_val_auc = 0\n","    pos_scores, neg_scores = [], []\n","    train_epoch_loss = 0\n","    valid_epoch_loss = 0\n","    for epoch in range(args.epochs):\n","        model.train()\n","        for step, (input_nodes, pos_pair_graph, neg_pair_graph, blocks) in enumerate(train_dataloader):\n","            # Forward\n","            pos_pair_graph = pos_pair_graph.to(device)\n","            neg_pair_graph = neg_pair_graph.to(device)\n","            blocks = [block.to(device) for block in blocks]\n","            input_features = blocks[0].srcdata[\"feat\"]\n","            # input_nodes = input_nodes.to(device)\n","\n","            pos_score, neg_score = model(input_features, pos_pair_graph, neg_pair_graph, blocks)\n","            train_loss = compute_loss(pos_score, neg_score)\n","            train_epoch_loss += train_loss.item()\n","\n","            # Compute accuracy on training/validation/test\n","            pos_scores.extend(pos_score)\n","            neg_scores.extend(neg_score)\n","\n","            # Backward\n","            optimizer.zero_grad()\n","            train_loss.backward()\n","            optimizer.step()\n","\n","        train_auc = compute_auc(pos_scores, neg_scores)\n","\n","        model.eval()\n","        with torch.no_grad():\n","            pos_scores, neg_scores = [], []\n","            for step, (input_nodes, pos_pair_graph, neg_pair_graph, blocks) in enumerate(valid_dataloader):\n","                pos_pair_graph = pos_pair_graph.to(device)\n","                neg_pair_graph = neg_pair_graph.to(device)\n","                blocks = [block.to(device) for block in blocks]\n","                # input_nodes = input_nodes.to(device)\n","                input_features = blocks[0].srcdata[\"feat\"]\n","                pos_score, neg_score = model(input_features, pos_pair_graph, neg_pair_graph, blocks)\n","                valid_loss = compute_loss(pos_score, neg_score)\n","                valid_epoch_loss += valid_loss.item()\n","                pos_scores.extend(pos_score)\n","                neg_scores.extend(neg_score)\n","\n","        val_auc = compute_auc(pos_scores, neg_scores)\n","\n","        # Save the best validation accuracy and the corresponding test accuracy.\n","        if best_val_auc < val_auc:\n","            best_val_auc = val_auc\n","            torch.save(model.state_dict(), args.save_name)\n","        if epoch % args.print_freq == 0:\n","            print(\n","                \"Epoch [{}] | loss: {:.3f}, val acc: {:.3f} (best {:.3f})\".format(\n","                    epoch, valid_loss, val_auc, best_val_auc\n","                )\n","            )\n","        # wandb.log({\"Train ACC\": train_acc, \"Epoch\": epoch})\n","        # wandb.log({\"Val ACC\": val_acc, \"Epoch\": epoch})\n","        # wandb.log({\"Train Loss\": train_loss, \"Epoch\": epoch})\n","        # wandb.log({\"Val Loss\": valid_loss, \"Epoch\": epoch})\n","\n","        scheduler.step(valid_loss)\n","\n","    # Testing\n","    state_dict = torch.load(args.save_name)\n","    model.load_state_dict(state_dict)\n","    model.eval()\n","\n","    trn_label, trn_pred = [], []\n","    with torch.no_grad():\n","        for step, (input_nodes, pos_pair_graph, neg_pair_graph, blocks) in enumerate(test_dataloader):\n","            pos_pair_graph = pos_pair_graph.to(device)\n","            neg_pair_graph = neg_pair_graph.to(device)\n","            blocks = [block.to(device) for block in blocks]\n","            input_features = blocks[0].srcdata[\"feat\"]\n","            # input_nodes = input_nodes.to(device)\n","            pos_score, neg_score = model(input_features, pos_pair_graph, neg_pair_graph, blocks)\n","            pos_scores.extend(pos_score)\n","            neg_scores.extend(neg_score)\n","\n","    test_acc = compute_auc(pos_scores, neg_scores)\n","\n","    print(f\"Test ACC: {test_acc}\")\n","    # wandb.log({\"Test ACC\": test_acc})\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"lVpQqUwJbnDR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685501065421,"user_tz":-480,"elapsed":88704,"user":{"displayName":"Dong Dong","userId":"17680164657657523368"}},"outputId":"26427940-6099-4fe5-b25a-7384b2d2559b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["  NumNodes: 2708\n","  NumEdges: 10556\n","  NumFeats: 1433\n","  NumClasses: 7\n","  NumTrainingSamples: 140\n","  NumValidationSamples: 500\n","  NumTestSamples: 1000\n","Done loading data from cached files.\n","Epoch [0] | loss: 1.472, val acc: 0.637 (best 0.637)\n","Epoch [1] | loss: 1.625, val acc: 0.634 (best 0.637)\n","Epoch [2] | loss: 1.594, val acc: 0.647 (best 0.647)\n","Epoch [3] | loss: 0.786, val acc: 0.768 (best 0.768)\n","Epoch [4] | loss: 0.630, val acc: 0.837 (best 0.837)\n","Epoch [5] | loss: 0.592, val acc: 0.853 (best 0.853)\n","Epoch [6] | loss: 0.452, val acc: 0.857 (best 0.857)\n","Epoch [7] | loss: 0.509, val acc: 0.869 (best 0.869)\n","Epoch [8] | loss: 0.492, val acc: 0.881 (best 0.881)\n","Epoch [9] | loss: 0.514, val acc: 0.891 (best 0.891)\n","Epoch [10] | loss: 0.233, val acc: 0.905 (best 0.905)\n","Epoch [11] | loss: 0.414, val acc: 0.905 (best 0.905)\n","Epoch [12] | loss: 0.432, val acc: 0.909 (best 0.909)\n","Epoch [13] | loss: 0.291, val acc: 0.913 (best 0.913)\n","Epoch [14] | loss: 0.525, val acc: 0.912 (best 0.913)\n","Epoch [15] | loss: 0.339, val acc: 0.923 (best 0.923)\n","Epoch [16] | loss: 0.479, val acc: 0.918 (best 0.923)\n","Epoch [17] | loss: 0.312, val acc: 0.928 (best 0.928)\n","Epoch [18] | loss: 0.307, val acc: 0.924 (best 0.928)\n","Epoch [19] | loss: 0.278, val acc: 0.928 (best 0.928)\n","Epoch [20] | loss: 0.330, val acc: 0.927 (best 0.928)\n","Epoch [21] | loss: 0.436, val acc: 0.925 (best 0.928)\n","Epoch [22] | loss: 0.403, val acc: 0.928 (best 0.928)\n","Epoch [23] | loss: 0.248, val acc: 0.923 (best 0.928)\n","Epoch [24] | loss: 0.298, val acc: 0.929 (best 0.929)\n","Epoch [25] | loss: 0.519, val acc: 0.927 (best 0.929)\n","Epoch [26] | loss: 0.506, val acc: 0.924 (best 0.929)\n","Epoch [27] | loss: 0.385, val acc: 0.924 (best 0.929)\n","Epoch [28] | loss: 0.424, val acc: 0.927 (best 0.929)\n","Epoch [29] | loss: 0.313, val acc: 0.925 (best 0.929)\n","Epoch [30] | loss: 0.499, val acc: 0.928 (best 0.929)\n","Epoch [31] | loss: 0.450, val acc: 0.928 (best 0.929)\n","Epoch [32] | loss: 0.537, val acc: 0.927 (best 0.929)\n","Epoch [33] | loss: 0.523, val acc: 0.928 (best 0.929)\n","Epoch [34] | loss: 0.498, val acc: 0.925 (best 0.929)\n","Epoch [35] | loss: 0.366, val acc: 0.927 (best 0.929)\n","Epoch [36] | loss: 0.481, val acc: 0.923 (best 0.929)\n","Epoch [37] | loss: 0.340, val acc: 0.928 (best 0.929)\n","Epoch [38] | loss: 0.340, val acc: 0.927 (best 0.929)\n","Epoch [39] | loss: 0.344, val acc: 0.926 (best 0.929)\n","Epoch [40] | loss: 0.464, val acc: 0.927 (best 0.929)\n","Epoch [41] | loss: 0.468, val acc: 0.926 (best 0.929)\n","Epoch [42] | loss: 0.432, val acc: 0.927 (best 0.929)\n","Epoch [43] | loss: 0.395, val acc: 0.925 (best 0.929)\n","Epoch [44] | loss: 0.367, val acc: 0.924 (best 0.929)\n","Epoch [45] | loss: 0.438, val acc: 0.923 (best 0.929)\n","Epoch [46] | loss: 0.370, val acc: 0.929 (best 0.929)\n","Epoch [47] | loss: 0.318, val acc: 0.926 (best 0.929)\n","Epoch [48] | loss: 0.356, val acc: 0.929 (best 0.929)\n","Epoch [49] | loss: 0.397, val acc: 0.929 (best 0.929)\n","Test ACC: 0.9230096149707302\n"]}]},{"cell_type":"markdown","source":["## 整圖分類 (Graph Classification)\n","\n","在整圖分類的任務中，我們批次輸入圖，再輸入過程中，我們使用 `GraphDataLoader` 批次讀取圖資料，並用 `torch.utils.data.sampler` 對批次的圖進行抽樣\n","\n","`Sampler` 是一個帶有 `__len__` 的 iterator，如果需要自定義 Sampler 需要包含三個部分\n","\n","1. `__init__:` 初始化設定\n","2. `__iter__:` 每次 iteration 中抽取 indices 的方法，回傳一個包含 indices 的 list，通常用 yield 方法\n","3. `__len__:` 數據長度\n"],"metadata":{"id":"kGy0D78Nx6t3"}},{"cell_type":"markdown","source":["### Example Dataset\n","\n","我們使用 `FakeNewsDataset`，該數據集來自於 Twitter 用戶的假新聞預測，介紹如下:\n","\n","該數據集由從 Twitter 提取的兩組樹狀結構的真實/虛假新聞傳播圖組成。與大多數用於圖分類任務的基準數據集不同，該數據集中的圖是有向樹狀結構圖，其中根節點代表新聞，葉節點是轉發根新聞的 Twitter 用戶。此外，節點特徵是使用不同的預訓練語言模型對用戶歷史推文進行編碼：\n","\n","- BERT: 768 維度\n","- content: 300 維度的 Spacy + 10 維度的 profile\n","- Spacy: 300 維度\n","- profile: 10 維度"],"metadata":{"id":"8KkIcqTf4dxG"}},{"cell_type":"code","source":["dataset = dgl.data.FakeNewsDataset('politifact', 'bert')\n","graph, label = dataset[1]\n","num_classes = dataset.num_classes\n","feat = dataset.feature\n","labels = dataset.labels\n","print(dataset)"],"metadata":{"id":"i4SxYRJaBMg0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685503748006,"user_tz":-480,"elapsed":1444,"user":{"displayName":"Dong Dong","userId":"17680164657657523368"}},"outputId":"2c39bb8b-f057-43be-e9f3-7a68db657f61"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset(\"politifact\", num_graphs=314, save_path=/root/.dgl/politifact_505d0a2f)\n"]}]},{"cell_type":"code","source":["graph.ndata"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9fXLf_Oc2Pww","executionInfo":{"status":"ok","timestamp":1685503749933,"user_tz":-480,"elapsed":260,"user":{"displayName":"Dong Dong","userId":"17680164657657523368"}},"outputId":"ab4e8160-303e-475f-84b7-704bca10b072"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'_ID': tensor([497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510,\n","        511, 512, 513, 514, 515, 516, 517])}"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["from dgl.dataloading import GraphDataLoader"],"metadata":{"id":"9XCxhzwe1NJ8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data.sampler import SubsetRandomSampler\n","\n","def _generate_dataloader(dataset, batch_size=16, train_size=.8, valid_size=.1, test_size=.1):\n","\n","    gids = np.arange(len(dataset))\n","    gids = np.random.permutation(gids)\n","\n","    overall_size = train_size + valid_size + test_size\n","    train_size = train_size / overall_size\n","    valid_size = valid_size / overall_size\n","    test_size = test_size / overall_size\n","\n","    train_size = int(len(gids) * train_size)\n","    valid_size = int(len(gids) * valid_size)\n","    test_size = int(len(gids) * test_size)\n","\n","    train_sampler = SubsetRandomSampler(torch.arange(train_size))\n","    valid_sampler = SubsetRandomSampler(torch.arange(train_size, train_size+valid_size))\n","    test_sampler = SubsetRandomSampler(torch.arange(train_size+valid_size, len(gids)))\n","\n","    train_dataloader = GraphDataLoader(\n","        dataset, sampler=train_sampler, batch_size=batch_size, drop_last=False\n","    )\n","    valid_dataloader = GraphDataLoader(\n","        dataset, sampler=valid_sampler, batch_size=batch_size, drop_last=False\n","    )\n","    test_dataloader = GraphDataLoader(\n","        dataset, sampler=test_sampler, batch_size=batch_size, drop_last=False\n","    )\n","\n","    return train_dataloader, valid_dataloader, test_dataloader"],"metadata":{"id":"pCSRl8Qg1YCO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from dgl.nn import GraphConv\n","\n","\n","class GCN(nn.Module):\n","    def __init__(self, in_feats, h_feats, num_classes):\n","        super(GCN, self).__init__()\n","        self.conv1 = GraphConv(in_feats, h_feats, allow_zero_in_degree=True)\n","        self.conv2 = GraphConv(h_feats, num_classes, allow_zero_in_degree=True)\n","\n","    def forward(self, g, in_feat):\n","        h = self.conv1(g, in_feat)\n","        h = F.relu(h)\n","        h = self.conv2(g, h)\n","        g.ndata[\"h\"] = h\n","        return dgl.mean_nodes(g, \"h\")"],"metadata":{"id":"iq0TPbWH1fuo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create the model with given dimensions\n","model = GCN(768, 100, 2)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n","\n","for epoch in range(20):\n","    for batched_graph, labels in train_dataloader:\n","        pred = model(batched_graph, feat[batched_graph.ndata[\"_ID\"]].float())\n","        loss = F.cross_entropy(pred, labels.long())\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","num_correct = 0\n","num_tests = 0\n","for batched_graph, labels in test_dataloader:\n","    pred = model(batched_graph, feat[batched_graph.ndata[\"_ID\"]].float().float())\n","    num_correct += (pred.argmax(1) == labels).sum().item()\n","    num_tests += len(labels)\n","\n","print(\"Test accuracy:\", num_correct / num_tests)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rBtpsglZ1o0z","executionInfo":{"status":"ok","timestamp":1685503942916,"user_tz":-480,"elapsed":10177,"user":{"displayName":"Dong Dong","userId":"17680164657657523368"}},"outputId":"1532267d-4c4a-499d-abff-a048b0322d41"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test accuracy: 0.7936507936507936\n"]}]},{"cell_type":"code","source":["# main.py\n","import argparse\n","import sys\n","from sklearn.metrics import accuracy_score\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","sys.argv = ['main.py']\n","\n","def parse_args():\n","    parser = argparse.ArgumentParser(description='DGL GNN Heterograph node classification')\n","    parser.add_argument('--epochs', type=int, default=50, help='number of training epochs')\n","    parser.add_argument('--batch_size', type=int, default=32, help='batch size')\n","    parser.add_argument(\"--print_freq\", type=int, default=1, help=\"print frequency\")\n","    parser.add_argument('--save_name', type=str, default='model.pth', help='name of the file to save the trained model')\n","    parser.add_argument('--lr', type=float, default=1e-2, help='learning rate for the optimizer')\n","    parser.add_argument('--weight_decay', type=float, default=0, help='weight decay for the optimizer')\n","    parser.add_argument('--resume', type=bool, default=False, help='resume or not')\n","    parser.add_argument('--resume_path', type=str, default=\"best_model.pth\", help='resume path')\n","    parser.add_argument('--num_classes', type=int, default=7, help='number of classes')\n","    parser.add_argument('--hidden_feats', type=int, default=16, help='number of hidden units')\n","    parser.add_argument('--model_name', type=str, default=\"tv_densenet121\", help='model name')\n","    parser.add_argument('--device', type=str, default=\"cuda\", help='device')\n","    parser.add_argument('--rank', type=int, default=0, help='the rank of the device, 0 if single GPU')\n","    parser.add_argument('--name', type=str, default=\"test\", help='name for the wandb run')\n","    parser.add_argument('--fanout', type=list, default=[10,25], help='number of samples in training steps')\n","    args = parser.parse_args()\n","    return args\n","\n","def main():\n","\n","    seed_everything(42)\n","\n","    args = parse_args()\n","\n","    # d = vars(args)\n","    # key = \"Your API key\"\n","    # wandb_settings(key, d, \"DGL-Cora-Heterograph-Node-Classification\", \"DDCVLAB\", args.name)\n","\n","    dataset = dgl.data.FakeNewsDataset('politifact', 'bert')\n","    num_classes = dataset.num_classes\n","    feat = dataset.feature\n","\n","    train_dataloader, valid_dataloader, test_dataloader = _generate_dataloader(dataset, args.batch_size)\n","\n","    model = GCN(feat.shape[1], args.hidden_feats, num_classes)\n","    if args.resume:\n","      state_dict = torch.load(args.resume_path)\n","      model.load_state_dict(state_dict, strict=True)\n","\n","    device = args.device\n","    model.to(device)\n","    feat = feat.to(device)\n","\n","    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n","    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=5)\n","\n","    best_val_auc = 0\n","    trn_label, trn_pred = [], []\n","    train_epoch_loss = 0\n","    valid_epoch_loss = 0\n","    for epoch in range(args.epochs):\n","        model.train()\n","        for step, (batched_graph, labels) in enumerate(train_dataloader):\n","            # Forward\n","            batched_graph = batched_graph.to(device)\n","            labels = labels.to(device)\n","            input_features = feat[batched_graph.ndata[\"_ID\"]].float()\n","\n","            logits = model(batched_graph, input_features)\n","            train_loss = F.cross_entropy(logits, labels.long())\n","            train_epoch_loss += train_loss.item()\n","\n","            # Compute accuracy on training/validation/test\n","            trn_label.extend(labels.cpu().numpy().tolist())\n","            trn_pred.extend(logits.argmax(1).cpu().numpy().tolist())\n","\n","            # Backward\n","            optimizer.zero_grad()\n","            train_loss.backward()\n","            optimizer.step()\n","\n","        train_auc = accuracy_score(trn_label, trn_pred)\n","\n","        model.eval()\n","        with torch.no_grad():\n","            trn_label, trn_pred = [], []\n","            for step, (batched_graph, labels) in enumerate(valid_dataloader):\n","\n","                batched_graph = batched_graph.to(device)\n","                labels = labels.to(device)\n","                input_features = feat[batched_graph.ndata[\"_ID\"]].float()\n","\n","                logits = model(batched_graph, input_features)\n","                valid_loss = F.cross_entropy(logits, labels.long())\n","                valid_epoch_loss += valid_loss.item()\n","                trn_label.extend(labels.cpu().numpy().tolist())\n","                trn_pred.extend(logits.argmax(1).cpu().numpy().tolist())\n","\n","        val_auc = accuracy_score(trn_label, trn_pred)\n","\n","        # Save the best validation accuracy and the corresponding test accuracy.\n","        if best_val_auc < val_auc:\n","            best_val_auc = val_auc\n","            torch.save(model.state_dict(), args.save_name)\n","        if epoch % args.print_freq == 0:\n","            print(\n","                \"Epoch [{}] | loss: {:.3f}, val acc: {:.3f} (best {:.3f})\".format(\n","                    epoch, valid_loss, val_auc, best_val_auc\n","                )\n","            )\n","        # wandb.log({\"Train ACC\": train_acc, \"Epoch\": epoch})\n","        # wandb.log({\"Val ACC\": val_acc, \"Epoch\": epoch})\n","        # wandb.log({\"Train Loss\": train_loss, \"Epoch\": epoch})\n","        # wandb.log({\"Val Loss\": valid_loss, \"Epoch\": epoch})\n","\n","        scheduler.step(valid_loss)\n","\n","    # Testing\n","    state_dict = torch.load(args.save_name)\n","    model.load_state_dict(state_dict)\n","    model.eval()\n","\n","    trn_label, trn_pred = [], []\n","    with torch.no_grad():\n","        for step, (batched_graph, labels) in enumerate(test_dataloader):\n","            batched_graph = batched_graph.to(device)\n","            labels = labels.to(device)\n","            input_features = feat[batched_graph.ndata[\"_ID\"]].float()\n","\n","            logits = model(batched_graph, input_features)\n","            trn_label.extend(labels.cpu().numpy().tolist())\n","            trn_pred.extend(logits.argmax(1).cpu().numpy().tolist())\n","\n","    test_acc = accuracy_score(trn_label, trn_pred)\n","\n","    print(f\"Test ACC: {test_acc}\")\n","    # wandb.log({\"Test ACC\": test_acc})\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8XBIZFEE1rI1","executionInfo":{"status":"ok","timestamp":1685505761652,"user_tz":-480,"elapsed":4832,"user":{"displayName":"Dong Dong","userId":"17680164657657523368"}},"outputId":"8ab7f4bf-19be-4c89-d72b-e749785799f0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [0] | loss: 0.904, val acc: 0.000 (best 0.000)\n","Epoch [1] | loss: 0.876, val acc: 0.000 (best 0.000)\n","Epoch [2] | loss: 0.909, val acc: 0.000 (best 0.000)\n","Epoch [3] | loss: 0.924, val acc: 0.000 (best 0.000)\n","Epoch [4] | loss: 0.815, val acc: 0.032 (best 0.032)\n","Epoch [5] | loss: 0.822, val acc: 0.065 (best 0.065)\n","Epoch [6] | loss: 0.860, val acc: 0.065 (best 0.065)\n","Epoch [7] | loss: 0.773, val acc: 0.290 (best 0.290)\n","Epoch [8] | loss: 0.767, val acc: 0.355 (best 0.355)\n","Epoch [9] | loss: 0.849, val acc: 0.290 (best 0.355)\n","Epoch [10] | loss: 0.538, val acc: 0.774 (best 0.774)\n","Epoch [11] | loss: 1.039, val acc: 0.129 (best 0.774)\n","Epoch [12] | loss: 0.392, val acc: 0.968 (best 0.968)\n","Epoch [13] | loss: 0.944, val acc: 0.258 (best 0.968)\n","Epoch [14] | loss: 0.351, val acc: 0.968 (best 0.968)\n","Epoch [15] | loss: 0.814, val acc: 0.419 (best 0.968)\n","Epoch [16] | loss: 0.423, val acc: 0.839 (best 0.968)\n","Epoch [17] | loss: 0.743, val acc: 0.484 (best 0.968)\n","Epoch [18] | loss: 0.410, val acc: 0.839 (best 0.968)\n","Epoch [19] | loss: 0.505, val acc: 0.742 (best 0.968)\n","Epoch [20] | loss: 0.490, val acc: 0.774 (best 0.968)\n","Epoch [21] | loss: 0.505, val acc: 0.742 (best 0.968)\n","Epoch [22] | loss: 0.487, val acc: 0.774 (best 0.968)\n","Epoch [23] | loss: 0.454, val acc: 0.774 (best 0.968)\n","Epoch [24] | loss: 0.473, val acc: 0.774 (best 0.968)\n","Epoch [25] | loss: 0.481, val acc: 0.774 (best 0.968)\n","Epoch [26] | loss: 0.482, val acc: 0.774 (best 0.968)\n","Epoch [27] | loss: 0.480, val acc: 0.774 (best 0.968)\n","Epoch [28] | loss: 0.479, val acc: 0.774 (best 0.968)\n","Epoch [29] | loss: 0.477, val acc: 0.774 (best 0.968)\n","Epoch [30] | loss: 0.477, val acc: 0.774 (best 0.968)\n","Epoch [31] | loss: 0.475, val acc: 0.774 (best 0.968)\n","Epoch [32] | loss: 0.475, val acc: 0.774 (best 0.968)\n","Epoch [33] | loss: 0.475, val acc: 0.774 (best 0.968)\n","Epoch [34] | loss: 0.475, val acc: 0.774 (best 0.968)\n","Epoch [35] | loss: 0.475, val acc: 0.774 (best 0.968)\n","Epoch [36] | loss: 0.474, val acc: 0.774 (best 0.968)\n","Epoch [37] | loss: 0.474, val acc: 0.774 (best 0.968)\n","Epoch [38] | loss: 0.474, val acc: 0.774 (best 0.968)\n","Epoch [39] | loss: 0.474, val acc: 0.774 (best 0.968)\n","Epoch [40] | loss: 0.474, val acc: 0.774 (best 0.968)\n","Epoch [41] | loss: 0.474, val acc: 0.774 (best 0.968)\n","Epoch [42] | loss: 0.474, val acc: 0.774 (best 0.968)\n","Epoch [43] | loss: 0.474, val acc: 0.774 (best 0.968)\n","Epoch [44] | loss: 0.474, val acc: 0.774 (best 0.968)\n","Epoch [45] | loss: 0.474, val acc: 0.774 (best 0.968)\n","Epoch [46] | loss: 0.474, val acc: 0.774 (best 0.968)\n","Epoch [47] | loss: 0.474, val acc: 0.774 (best 0.968)\n","Epoch [48] | loss: 0.474, val acc: 0.774 (best 0.968)\n","Epoch [49] | loss: 0.474, val acc: 0.774 (best 0.968)\n","Test ACC: 0.84375\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"rNsYjXAY9Rti"},"execution_count":null,"outputs":[]}]}